debug: false
enable_cuda_optimizations: true
full_fit: false
use_wandb: false
seed: 1989
local_rank: #运行自动生成
fold: 0
use_deepspeed_plugin: true

task:
  name: base                     # 当前运行的任务名称: qcot, semi, hard 之一
  chain:                         # 定义任务执行顺序，便于确定前一个任务
    - qcot
    - semi
    - hard
  first_task_name: qcot          # 第一个任务名称

dataset:
  query_dataset: ../input/synthetic.csv
  content_dataset: ../input/eedi_content.csv
  folder_dataset: ../input/fold_df.csv
  ext_cot_datasdet: ../input/json_pack_full.json.gz
  negative_dataset: ...


model:
  name: bge_embedding
  model_name: BGE-large-en

  base_backbone_path: ../models/bge/base_model  # LoRA模式下的公共基础模型路径
  base_backbone_name: BAAI/bge-large-en-v1.5  # HuggingFace模型名，用于初始化
  
  model_type: bge_embedding
  trust_remote_code: false
  max_length: 512
  sentence_pooling_method: cls
  gradient_checkpointing: true
  compile: false
  attn_implementation: sdpa # bert 不支持 flash_attention_2
  negatives_cross_device: false
  padding_side: right
  add_eos_token: false
  n_neighbour: 256

  use_bnb: false
  use_lora: true
  lora:
    r: 4  # 低秩近似的秩
    lora_alpha: 16  # LoRA适配器的缩放因子
    lora_dropout: 0.1  # Dropout率
    target_modules:
      - query      # 匹配所有包含 "self.query" 的路径
      - key
      - value
      - encoder.layer.20.output.dense
      - encoder.layer.21.output.dense
      - encoder.layer.22.output.dense
      - encoder.layer.23.output.dense
      # - dense
    modules_to_save: []  # 确保不保留任何原始模块
        # "encoder.layer.20.output.dense",
        # "encoder.layer.21.output.dense",
        # "encoder.layer.22.output.dense",
        # "encoder.layer.23.output.dense",
train_params:
  retriever_bs: 128 #memory ~ bs * num_negative or bs(if negative_dataset ==0)
  sub_batch_size: 4 # emebding memory
  query_bs: 128 # evaluate
  content_bs: 128 # evaluate

  num_negative: 0
  warmup_pct: 0.1
  num_epochs: 10
  grad_accumulation_steps: 1
  patience: 3
  eval_at_start: false

  
  loss:
    alpha: 0.3
    beta: 0.5
    gamma: 0.2
optimizer:
  name: AdamW
  head_keywords: headmodel
  
  lr:  1e-5
  lr_lora_a:  1e-5
  lr_lora_b: 5e-5
  lr_embed_tokens: 8e-5
  lr_head: 3e-4 

  max_grad_norm: 1
  adam_beta_1: 0.9
  adam_beta_2: 0.95
  adam_epsilon: 1e-8
  weight_decay: 1e-2
# 前10%训练步：max_grad_norm=0.5（严格稳定）

# 中间50%训练步：max_grad_norm=1.0（平衡）

# 后40%训练步：max_grad_norm=2.0（加速收敛）
outputs:
  model_dir: ../models/bge/output