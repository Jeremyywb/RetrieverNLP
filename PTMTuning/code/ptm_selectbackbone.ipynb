{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4096a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer,AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52bbe1b-043c-4755-9033-257f486628f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"D:\\COMPETITIONS\\RetrieverNLP\\PTMTuning\\input\\eedi_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c2fbf-8ae7-4786-b3e8-efd778753dc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9d5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch['query']['input_ids'].shape)  # Should be (2, 3)\n",
    "batch['contents']['input_ids'].shape  # Should be (2, 2, 3) (2 samples, 2 contents each, 3 tokens each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8aa236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[ 1,  2,  3],\n",
       "          [ 4,  5,  6]],\n",
       " \n",
       "         [[ 7,  8,  9],\n",
       "          [10, 11, 12]]]),\n",
       " 'attention_mask': tensor([[[1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       " \n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9befca67-5b02-401f-83cc-51d05f17d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS_TO_TEST = {\n",
    "#     \"gte-Qwen2-7B-instruct\": {\n",
    "#         \"model_name\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "#         \"query_instruction\": \"\",\n",
    "#         \"normalize\": False,\n",
    "#         \"trust_remote_code\": True,  # 必须启用\n",
    "#         \"model_class\": \"AutoModelForCausalLM\"  # 指定生成式模型接口\n",
    "#     }\n",
    "# }\n",
    "# model_config = MODELS_TO_TEST['gte-Qwen2-7B-instruct']\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_config[\"model_name\"],\n",
    "#         trust_remote_code=True,\n",
    "#         device_map=\"auto\",\n",
    "#         torch_dtype=torch.float16\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1771224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CFG:\n",
    "    input_path = \"/root/cloud/RetrieverNLP/PTM_tuning/input/\"\n",
    "    train_path = f\"{input_path}train.csv\"\n",
    "    test_path  = f\"{input_path}test.csv\"\n",
    "    misc_path  = f\"{input_path}misconception_mapping.csv\"\n",
    "    samp_path  = f\"{input_path}sample_submission.csv\"\n",
    "#     max_cutoff = 50 #V1\n",
    "    #max_cutoff = 100 #V2\n",
    "    #max_cutoff = 50 #v3\n",
    "    max_cutoff = 150 #v3\n",
    "    \n",
    "    is_train   = False\n",
    "    if is_train:\n",
    "        embd_name  = \"BAAI/bge-large-en-v1.5\"#online\n",
    "        rerank_na  = 'BAAI/bge-reranker-large'#online\n",
    "    else:\n",
    "        embd_name  = \"/kaggle/input/bge-large-en-v1-5/bge-large-en-v1.5\"#offline\n",
    "        rerank_na  = \"/kaggle/input/bge-reranker-large\"#offline\n",
    "        \n",
    "    with_fineture_reranker = True\n",
    "    reranker_fineture_path = \"/kaggle/input/bge-reranker-ft-v2\"\n",
    "#     reranker_fineture_path = \"/kaggle/input/bge-reranker-ft-v3\"\n",
    "cfg = CFG()\n",
    "\n",
    "\n",
    "train                 = pd.read_csv(cfg.train_path)\n",
    "misconception_mapping = pd.read_csv(cfg.misc_path)\n",
    "\n",
    "\n",
    "\n",
    "def make_all_question_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"all_question_text\"] = df[\"ConstructName\"] +\" \" +df[\"QuestionText\"]\n",
    "    return df\n",
    "train = make_all_question_text(train)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 第一步：处理答案文本（AnswerXText）\n",
    "    text_long = pd.melt(\n",
    "        df,\n",
    "        id_vars    = [\"QuestionId\", \"all_question_text\", \"CorrectAnswer\"],\n",
    "        value_vars = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"],\n",
    "        var_name   = 'Answer',\n",
    "        value_name = 'AnswerText'\n",
    "    )\n",
    "    # 提取答案选项字母（如 A/B/C/D）\n",
    "    text_long['Answer'] = text_long['Answer'].str.replace('Answer', '').str.replace('Text', '')\n",
    "\n",
    "    # 第二步：处理错误概念ID（MisconceptionXId）\n",
    "    misconception_long = pd.melt(\n",
    "        df,\n",
    "        id_vars    = [\"QuestionId\"],\n",
    "        value_vars = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"],\n",
    "        var_name   = 'MisconceptionAnswer',\n",
    "        value_name = 'MisconceptionId'\n",
    "    )\n",
    "    # 提取答案选项字母（如 A/B/C/D）\n",
    "    misconception_long['Answer'] = misconception_long['MisconceptionAnswer'].str.replace('Misconception', '').str.replace('Id', '')\n",
    "    misconception_long = misconception_long.drop(columns=['MisconceptionAnswer'])\n",
    "\n",
    "    # 合并两个长格式数据\n",
    "    merged_long = pd.merge(\n",
    "        text_long,\n",
    "        misconception_long,\n",
    "        on=['QuestionId', 'Answer'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return merged_long\n",
    "\n",
    "train_long = wide_to_long(train)\n",
    "\n",
    "def make_all_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"all_text\"] = df[\"all_question_text\"] +\" \" +df[\"Answer\"]\n",
    "    df = df[ (df.CorrectAnswer!=df.Answer)&(df.MisconceptionId.isnull()==False) ].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_long = make_all_text(train_long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ec79a-5260-49d0-b071-fe334a24952e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c2aece-4877-4d33-8ec1-4c1b02964e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating multilingual-e5-large ===\n",
      "Loading model: intfloat/multilingual-e5-large\n",
      "使用AutoModel加载嵌入模型\n",
      "模型加载成功: intfloat/multilingual-e5-large\n",
      "编码候选池文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:01<00:00, 42.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码查询文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:16<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算Recall@50...\n",
      "multilingual-e5-large Recall@50: 41.56%\n",
      "\n",
      "=== Evaluating snowflake-arctic-embed-l-v2.0 ===\n",
      "Loading model: Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "使用AutoModel加载嵌入模型\n",
      "模型加载成功: Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "编码候选池文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:01<00:00, 41.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码查询文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:16<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算Recall@50...\n",
      "snowflake-arctic-embed-l-v2.0 Recall@50: 62.86%\n",
      "\n",
      "=== Final Results ===\n",
      "snowflake-arctic-embed-l-v2.0 | Recall@50: 62.86%\n",
      "multilingual-e5-large | Recall@50: 41.56%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# 配置区（按需修改）\n",
    "# ======================================================\n",
    "\t\n",
    "# bge-en-icl\n",
    "MODELS_TO_TEST = {\n",
    "\n",
    "    \"BGE-large-en\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"query_instruction\": \"Represent this sentence for searching relevant passages: \",\n",
    "        \"normalize\": True,\n",
    "        \"trust_remote_code\":False\n",
    "    },\n",
    "    \"BGE-M3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"query_instruction\": \"\",\n",
    "        \"normalize\": False , # M3自带归一化\n",
    "        \"trust_remote_code\":False\n",
    "    },\n",
    "\n",
    "    \n",
    "     \"jina-embeddings-v3\": {\n",
    "        \"model_name\": \"jinaai/jina-embeddings-v3\",\n",
    "        \"query_instruction\": \"\",\n",
    "        \"normalize\": True , \n",
    "        \"trust_remote_code\":True  \n",
    "    },\n",
    "    \n",
    "    \"snowflake-arctic-embed-l-v2.0\": {\n",
    "            \"model_name\": \"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
    "            \"query_instruction\": \"\",\n",
    "            \"normalize\": True , \n",
    "            \"trust_remote_code\":False  \n",
    "        },\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PER_GPU_BATCH_SIZE = 32\n",
    "\n",
    "# ======================================================\n",
    "# 核心评估函数\n",
    "# ======================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 新版本加载模型代码，解决KeyError: 'mistral'问题\n",
    "def evaluate_model(model_config, misconception_mapping, train_data):\n",
    "    \"\"\"评估单个模型的Recall@50\"\"\"\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    trust_remote_code = model_config.get(\"trust_remote_code\", False)\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    PER_GPU_BATCH_SIZE = 32\n",
    "    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 尝试先判断模型类型\n",
    "        if \"Qwen\" in model_name:\n",
    "            print(\"识别为Qwen生成模型，使用AutoModelForCausalLM加载\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=trust_remote_code,\n",
    "                torch_dtype=torch.float16 if \"7B\" in model_name else torch.float32,  # 大模型使用半精度\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=trust_remote_code\n",
    "            )\n",
    "        elif model_name=='snowflake-arctic-embed-l-v2.0':\n",
    "            print(\"使用AutoModel加载嵌入模型\")\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=trust_remote_code\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                add_pooling_layer=False,\n",
    "                trust_remote_code=trust_remote_code\n",
    "            )\n",
    "        else:\n",
    "            # 嵌入模型默认使用AutoModel\n",
    "            print(\"使用AutoModel加载嵌入模型\")\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=trust_remote_code\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=trust_remote_code\n",
    "            )\n",
    "        \n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "        print(f\"模型加载成功: {model_name}\")\n",
    "        \n",
    "    except KeyError as e:\n",
    "        if \"mistral\" in str(e):\n",
    "            print(\"检测到mistral模型加载错误。尝试使用特殊处理...\")\n",
    "            # 针对mistral模型的特殊处理\n",
    "            try:\n",
    "                from transformers import MistralForCausalLM, MistralConfig\n",
    "                \n",
    "                print(\"使用MistralForCausalLM直接加载\")\n",
    "                model = MistralForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    trust_remote_code=trust_remote_code,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "                )\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_name,\n",
    "                    trust_remote_code=trust_remote_code\n",
    "                )\n",
    "                model = model.to(DEVICE)\n",
    "                print(f\"Mistral模型加载成功: {model_name}\")\n",
    "            except Exception as mistral_error:\n",
    "                print(f\"Mistral特殊处理失败: {mistral_error}\")\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # 构建候选池索引映射\n",
    "    misconception_ids = misconception_mapping['MisconceptionId'].values.tolist()\n",
    "    misconception_texts = misconception_mapping['MisconceptionName'].values.tolist()\n",
    "    id_to_index = {_id: idx for idx, _id in enumerate(misconception_ids)}\n",
    "\n",
    "    # 生成候选池向量\n",
    "    def encode_texts(texts, is_query=False):\n",
    "        all_vectors = []\n",
    "        for i in tqdm(range(0, len(texts), PER_GPU_BATCH_SIZE)):\n",
    "            batch_texts = texts[i:i+PER_GPU_BATCH_SIZE]\n",
    "            \n",
    "            # 添加指令前缀（如果需要）\n",
    "            if is_query and model_config.get(\"query_instruction\", \"\"):\n",
    "                batch_texts = [model_config[\"query_instruction\"] + t for t in batch_texts]\n",
    "            \n",
    "            # 准备输入\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # 提取嵌入向量\n",
    "            with torch.no_grad():\n",
    "                if isinstance(model, AutoModelForCausalLM.__bases__[0]) or \"CausalLM\" in model.__class__.__name__:\n",
    "                    # 对于生成模型，使用隐藏状态作为嵌入\n",
    "                    outputs = model(**inputs, output_hidden_states=True)\n",
    "                    # 对于生成模型，使用最后一层隐藏状态的平均作为嵌入\n",
    "                    embeddings = outputs.hidden_states[-1].mean(dim=1)\n",
    "                else:\n",
    "                    # 对于嵌入模型，使用标准方法\n",
    "                    outputs = model(**inputs)\n",
    "                    embeddings = outputs.last_hidden_state[:, 0]  # 使用CLS token\n",
    "            \n",
    "            # 归一化（如果需要）\n",
    "            if model_config.get(\"normalize\", False):\n",
    "                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "            # all_vectors.append(embeddings.cpu().numpy())\n",
    "            all_vectors.append(embeddings.cpu().to(torch.float32).numpy())\n",
    "        \n",
    "        return np.concatenate(all_vectors, axis=0)\n",
    "    \n",
    "    # 编码候选池\n",
    "    print(\"编码候选池文本...\")\n",
    "    sentence_embeddings = encode_texts(misconception_texts)\n",
    "    \n",
    "    # 编码查询\n",
    "    print(\"编码查询文本...\")\n",
    "    query_texts = [item[\"all_text\"] for _, item in train_data.iterrows()]\n",
    "    ground_truth_indices = [id_to_index[item[\"MisconceptionId\"]] for _, item in train_data.iterrows()]\n",
    "    query_embeddings = encode_texts(query_texts, is_query=True)\n",
    "    \n",
    "    # 计算Recall@50\n",
    "    def calculate_recall(sent_emb, query_emb, gt_indices, top_k=50):\n",
    "        index = faiss.IndexFlatIP(sent_emb.shape[1])\n",
    "        index.add(sent_emb.astype(np.float32))\n",
    "        \n",
    "        _, top_indices = index.search(query_emb.astype(np.float32), top_k)\n",
    "        return np.mean([1 if gt in indices else 0 for gt, indices in zip(gt_indices, top_indices)])\n",
    "    \n",
    "    print(\"计算Recall@50...\")\n",
    "    return calculate_recall(sentence_embeddings, query_embeddings, ground_truth_indices)\n",
    "\n",
    "\n",
    "# 使用示例:\n",
    "# results = {}\n",
    "# for model_name, config in MODELS_TO_TEST.items():\n",
    "#     print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "#     recall = evaluate_model(config, misconception_mapping, train_long)\n",
    "#     results[model_name] = recall\n",
    "#     print(f\"{model_name} Recall@50: {recall*100:.2f}%\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 执行测试\n",
    "# ======================================================\n",
    "\n",
    "    # 加载数据（需替换为实际数据）\n",
    "    # misconception_mapping = ... \n",
    "    # train_data = ...\n",
    "\n",
    "\n",
    "results = {}\n",
    "for model_name, config in MODELS_TO_TEST.items():\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    recall = evaluate_model(config, misconception_mapping, train_long)\n",
    "    results[model_name] = recall\n",
    "    print(f\"{model_name} Recall@50: {recall*100:.2f}%\")\n",
    "\n",
    "# 打印最终对比结果\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for model, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model:<15} | Recall@50: {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b9738-c3ad-4024-b6e0-16a98dacdb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  query_id  content_id                                        explanation  \\\n",
      "0      0_D        1672  Correct calculation: 3×(2+4)-5 = 3×6-5 = 18-5 ...   \n",
      "1   1000_A         891  Correct Calculation:\\n\\( \\frac{1-t}{t-1} = \\fr...   \n",
      "2   1000_D         353  Correct calculation:\\n\\[\\n\\frac{1-t}{t-1} = \\f...   \n",
      "3   1002_B        1715  To convert \\( \\frac{3}{20} \\) to a decimal, th...   \n",
      "4   1002_C        2308  Correct calculation: \\( 3 \\div 20 = 0.15 \\).  ...   \n",
      "\n",
      "                    timestamp error  \n",
      "0  2025-04-20T22:00:24.440664  None  \n",
      "1  2025-04-20T22:00:24.440664  None  \n",
      "2  2025-04-20T22:00:24.440664  None  \n",
      "3  2025-04-21T21:23:04.159306  None  \n",
      "4  2025-04-20T22:00:24.440664  None  \n"
     ]
    }
   ],
   "source": [
    "# === Final Results ===\n",
    "# BGE-large-en    | Recall@50: 64.51%\n",
    "# BGE-M3          | Recall@50: 59.04%\n",
    "# jina-embeddings-v3 Recall@50: 67.12%\n",
    "# multilingual-e5-large Recall@50: 41.56%\n",
    "# snowflake-arctic-embed-l-v2.0 | Recall@50: 62.86%\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_json_gz_to_dataframe(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    从 .json.gz 文件中读取每行一个 JSON 对象，转换为 pandas DataFrame。\n",
    "\n",
    "    参数:\n",
    "        file_path: 压缩 JSON 文件路径\n",
    "\n",
    "    返回:\n",
    "        pandas DataFrame\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    data.append(obj)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # 跳过格式错误的行\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 示例用法\n",
    "df = load_json_gz_to_dataframe('../input/json_pack_full.json.gz')\n",
    "print(df.head())\n",
    "index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "053e7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['query_id', 'content_id', 'hard_negatives', 'hard_negative_count'], dtype='object')\n",
      "Index(['MisconceptionId', 'MisconceptionName'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 'Does not know that angles in a triangle sum to 180 degrees'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "DATAPATH = '../input/negatives_hard.csv'\n",
    "negatives_hard = pd.read_csv(DATAPATH)  \n",
    "print(negatives_hard.iloc[0].keys())\n",
    "negatives_hard.iloc[0].values\n",
    "\n",
    "DATAPATH = '../input/eedi_content.csv'\n",
    "eedi_content = pd.read_csv(DATAPATH)  \n",
    "print(eedi_content.iloc[0].keys())\n",
    "eedi_content.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4217158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>hard_negatives</th>\n",
       "      <th>hard_negative_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84_A</td>\n",
       "      <td>1568</td>\n",
       "      <td>2301,642,2127,2113,1144,1304,2080,287,1285,1139</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1591_A</td>\n",
       "      <td>2450</td>\n",
       "      <td>2197,1788,2353,192,305,2388,648,396,867,910</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>885_A</td>\n",
       "      <td>1877</td>\n",
       "      <td>226,926,551,1007,2355,561,2394,1851,1370,465</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59_C</td>\n",
       "      <td>2206</td>\n",
       "      <td>1601,1313,247,1965,2191,914,1710,2023,2112,531</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1193_C</td>\n",
       "      <td>2362</td>\n",
       "      <td>309,2121,100,48,2117,356,793,1707,725,2334</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>587_A</td>\n",
       "      <td>706</td>\n",
       "      <td>15,2306,1507,1005,1963,1516,2140,561,987,1999</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>355_D</td>\n",
       "      <td>926</td>\n",
       "      <td>2355,1877,1007,1687,561,1399,1370,1791,515,2161</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>983_C</td>\n",
       "      <td>234</td>\n",
       "      <td>1332,2414,1514,997,1887,2093,1648,1219,835,1978</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>1306_C</td>\n",
       "      <td>2275</td>\n",
       "      <td>118,1014,1873,836,2416,745,761,2427,1543,1340</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>1202_A</td>\n",
       "      <td>1196</td>\n",
       "      <td>1294,314,511,340,34,1219,1729,1266,2291,94</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2975 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id  content_id                                   hard_negatives  \\\n",
       "0        84_A        1568  2301,642,2127,2113,1144,1304,2080,287,1285,1139   \n",
       "1      1591_A        2450      2197,1788,2353,192,305,2388,648,396,867,910   \n",
       "2       885_A        1877     226,926,551,1007,2355,561,2394,1851,1370,465   \n",
       "3        59_C        2206   1601,1313,247,1965,2191,914,1710,2023,2112,531   \n",
       "4      1193_C        2362       309,2121,100,48,2117,356,793,1707,725,2334   \n",
       "...       ...         ...                                              ...   \n",
       "2970    587_A         706    15,2306,1507,1005,1963,1516,2140,561,987,1999   \n",
       "2971    355_D         926  2355,1877,1007,1687,561,1399,1370,1791,515,2161   \n",
       "2972    983_C         234  1332,2414,1514,997,1887,2093,1648,1219,835,1978   \n",
       "2973   1306_C        2275    118,1014,1873,836,2416,745,761,2427,1543,1340   \n",
       "2974   1202_A        1196       1294,314,511,340,34,1219,1729,1266,2291,94   \n",
       "\n",
       "      hard_negative_count  \n",
       "0                      10  \n",
       "1                      10  \n",
       "2                      10  \n",
       "3                      10  \n",
       "4                      10  \n",
       "...                   ...  \n",
       "2970                   10  \n",
       "2971                   10  \n",
       "2972                   10  \n",
       "2973                   10  \n",
       "2974                   10  \n",
       "\n",
       "[2975 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d20ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_9516\\3745703134.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m negatives_hard.groupby(\u001b[33m\"query_id\"\u001b[39m)[\u001b[33m\"hard_negatives\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [x.split(\u001b[33m\",\"\u001b[39m)]).to_dict()\n",
      "\u001b[32md:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m             input=\u001b[33m\"series\"\u001b[39m, examples=_apply_docs[\u001b[33m\"series_examples\"\u001b[39m]\n\u001b[32m    227\u001b[39m         )\n\u001b[32m    228\u001b[39m     )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply(self, func, *args, **kwargs) -> Series:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m super().apply(func, *args, **kwargs)\n",
      "\u001b[32md:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1833\u001b[39m                         ),\n\u001b[32m   1834\u001b[39m                         category=DeprecationWarning,\n\u001b[32m   1835\u001b[39m                         stacklevel=find_stack_level(),\n\u001b[32m   1836\u001b[39m                     )\n\u001b[32m-> \u001b[39m\u001b[32m1837\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m TypeError:\n\u001b[32m   1838\u001b[39m                 \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[32m   1839\u001b[39m                 \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[32m   1840\u001b[39m                 \u001b[38;5;66;03m# operation, by excluding the grouping column\u001b[39;00m\n",
      "\u001b[32md:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1881\u001b[39m         -------\n\u001b[32m   1882\u001b[39m         Series \u001b[38;5;28;01mor\u001b[39;00m DataFrame\n\u001b[32m   1883\u001b[39m             data after applying f\n\u001b[32m   1884\u001b[39m         \"\"\"\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m         values, mutated = self._grouper.apply_groupwise(f, data, self.axis)\n\u001b[32m   1886\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m             not_indexed_same = mutated\n\u001b[32m   1888\u001b[39m \n",
      "\u001b[32md:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    915\u001b[39m             object.__setattr__(group, \u001b[33m\"name\"\u001b[39m, key)\n\u001b[32m    916\u001b[39m \n\u001b[32m    917\u001b[39m             \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m             group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m             res = f(group)\n\u001b[32m    920\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m mutated \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m                 mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    922\u001b[39m             result_values.append(res)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_9516\\3745703134.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m negatives_hard.groupby(\u001b[33m\"query_id\"\u001b[39m)[\u001b[33m\"hard_negatives\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [x.split(\u001b[33m\",\"\u001b[39m)]).to_dict()\n",
      "\u001b[32md:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6295\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6296\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m         ):\n\u001b[32m   6298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "negatives_hard.groupby(\"query_id\")[\"hard_negatives\"].apply(lambda x: x.split(\",\")).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "becac815",
   "metadata": {},
   "outputs": [],
   "source": [
    "eedi_content['MisconceptionId'] = eedi_content['MisconceptionId'].astype(str)\n",
    "eedi_content_dict = eedi_content.set_index('MisconceptionId')['MisconceptionName'].to_dict()\n",
    "\n",
    "def get_content_of_hardnage_byquery_id(query_id, eedi_content_dict, negatives_hard):\n",
    "    \"\"\"\n",
    "    根据查询ID获取相关的hardneg内容\n",
    "    :param query_id: 查询ID\n",
    "    :param eedi_content: eedi_content DataFrame\n",
    "    :param negatives_hard: hardnegatives DataFrame\n",
    "    :return: 相关的hardneg内容\n",
    "    \"\"\"\n",
    "    # 获取与查询ID匹配的hardneg内容\n",
    "    # row_to_dict\n",
    "    if query_id not in negatives_hard['query_id'].values:\n",
    "        print(\"当前的query_id在hardnegatives中不存在\")\n",
    "        return []\n",
    "    query_row_to_dict = negatives_hard[negatives_hard['query_id'] == query_id].to_dict(orient='records')[0]\n",
    "    current_content_id = query_row_to_dict['content_id']\n",
    "    \n",
    "    # 获取hardneg内容的ID列表\n",
    "    hardneg_ids = query_row_to_dict['hard_negatives'].split(\",\")\n",
    "    \n",
    "    # 根据ID从eedi_content中获取对应的内容\n",
    "    related_contents = [eedi_content_dict.get(hardneg_id.strip(), '未获取到相关内容') for hardneg_id in hardneg_ids]\n",
    "    current_contents = eedi_content_dict.get(str(current_content_id), '未获取到相关内容')\n",
    "    return related_contents,current_contents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e794d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_stratified_folds\u001b[39m(query_df, n_folds=\u001b[32m5\u001b[39m):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    生成分层交叉验证的fold，保证：\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    1. 相同original_query_id的样本在同一fold\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33;03m    包含fold_id的DataFrame\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b205f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting sklearn\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/46/1c/395a83ee7b2d2ad7a05b453872053d41449564477c81dc356f720b16eac4/sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e71b56f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前content内容: Thinks rectangles have rotational symmetry or order 4\n",
      "Related content 1: Thinks rectangles have line symmetry in the diagonals \n",
      "Related content 2: Believes the number of sides gives the order of rotational symmetry\n",
      "Related content 3: Believes a rectangle has four equal sides\n",
      "Related content 4: Does not connect rotational symmetry to number of sides/vertices\n",
      "Related content 5: Does not know how to find order of rotational symmetry\n",
      "Related content 6: Misinterprets the order of division in worded problems\n",
      "Related content 7: Does not know the properties of an equilateral triangle\n",
      "Related content 8: Does not know the properties of an isosceles triangle\n",
      "Related content 9: Does not believe a shape can have two correct names e.g. rectangle and parallelogram\n",
      "Related content 10: Does not know the diagonals of a square bisect at right angles\n"
     ]
    }
   ],
   "source": [
    "qid = '885_A'\n",
    "related_contents,current_content = get_content_of_hardnage_byquery_id(qid, eedi_content_dict, negatives_hard)\n",
    "print('当前content内容:', current_content)\n",
    "for i, content in enumerate(related_contents):\n",
    "    print(f\"Related content {i+1}: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499663b-f0eb-4dab-865d-7003a926a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "# !pip install einops\n",
    "# !rm -rf ~/.cache/huggingface/transformers_modules/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16b7f2-cfd5-4cfd-b8c3-985dd06025fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
