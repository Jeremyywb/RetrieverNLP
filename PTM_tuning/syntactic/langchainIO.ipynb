{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdf3935-fbc6-4ab7-986b-55442c89f583",
   "metadata": {},
   "source": [
    "## *PDF解析-marker*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e798b-a05c-4f36-9180-5992dd2e6843",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-12T09:16:32.291906Z",
     "iopub.status.busy": "2025-04-12T09:16:32.291624Z",
     "iopub.status.idle": "2025-04-12T09:16:35.863470Z",
     "shell.execute_reply": "2025-04-12T09:16:35.863046Z",
     "shell.execute_reply.started": "2025-04-12T09:16:32.291890Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: dashscope in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (1.22.2)\n",
      "Requirement already satisfied: aiohttp in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from dashscope) (3.11.14)\n",
      "Requirement already satisfied: requests in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from dashscope) (2.32.3)\n",
      "Requirement already satisfied: websocket-client in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from dashscope) (1.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp->dashscope) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests->dashscope) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests->dashscope) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests->dashscope) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests->dashscope) (2025.1.31)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting asyncio\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/22/74/07679c5b9f98a7cb0fc147b1ef1cc1853bc07a4eb9cb5731e24732c5f773/asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "Installing collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: langchain in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (0.3.47)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: langchain_openai in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_openai) (0.3.47)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.66.3 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_openai) (1.68.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (0.3.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain_openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain_openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain_openai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain_openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.45->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.45->langchain_openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.66.3->langchain_openai) (0.4.6)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: langchain_community in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (0.3.47)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (0.3.18)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\competitions\\langchainexam\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %env DASHSCOPE_API_KEY=sk-a6ba756d766e41b3afd31c57392b7575\n",
    "\n",
    "# from langchain_community.llms.tongyi import Tongyi  \n",
    "# from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate  \n",
    "  \n",
    "# examples = [  \n",
    "#     {  \n",
    "#         \"question\": \"罗杰有五个网球，他又买了两盒网球，每盒有3个网球，请问他现在总共有多少个网球？\",  \n",
    "#         \"answer\": \"罗杰一开始有五个网球，又购买了两盒网球，每盒3个，共购买了6个网球，因此现在总共由5+6=11个网球。因此答案是11。\"  \n",
    "#     },  \n",
    "#     {  \n",
    "#         \"question\": \"食堂总共有23个苹果，如果他们用掉20个苹果，然后又买了6个苹果，请问现在食堂总共有多少个苹果？\",  \n",
    "#         \"answer\": \"食堂最初有23个苹果，用掉20个，然后又买了6个，总共有23-20+6=9个苹果，答案是9。\"  \n",
    "#     },  \n",
    "#     {  \n",
    "#         \"question\": \"杂耍者可以杂耍16个球。其中一半的球是高尔夫球，其中一半的高尔夫球是蓝色的。请问总共有多少个蓝色高尔夫球？\",  \n",
    "#         \"answer\": \"总共有16个球，其中一半是高尔夫球，也就是8个，其中一半是蓝色的，也就是4个，答案是4个。\"  \n",
    "#     },  \n",
    "# ]  \n",
    "# example_prompt = PromptTemplate(  \n",
    "#     input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n Answer:{answer}\"  \n",
    "# )\n",
    "# few_shot_prompt = FewShotPromptTemplate(  \n",
    "#     examples=examples,  \n",
    "#     example_prompt=example_prompt,  \n",
    "#     suffix=\"Question: {input}\",  # 后缀模板，其中 {input} 会被替换为实际输入  \n",
    "#     input_variables=[\"input\"]  # 定义输入变量的列表  \n",
    "# )\n",
    "# question = '艾米需要4分钟才能爬到滑梯顶部，她花了1分钟才滑下来，水滑梯将在15分钟后关闭，请问在关闭之前她能滑多少次？'  \n",
    "# prompt_format = few_shot_prompt.format(input=question)\n",
    "# llm = Tongyi()\n",
    "# result = llm.invoke(prompt_format)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# !pip install dashscope\n",
    "# !pip install asyncio\n",
    "# !pip install langchain\n",
    "# !pip install langchain_openai\n",
    "# !pip install langchain_community\n",
    "\n",
    "# !pip uninstall torch torchvision -y\n",
    "\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# !pip install marker-pdf\n",
    "# !marker_single  Day+2：模型I_O与数据连接.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9055e5-d8d5-414d-8635-76458b9f99eb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-12T09:15:49.372253Z",
     "iopub.status.busy": "2025-04-12T09:15:49.371933Z",
     "iopub.status.idle": "2025-04-12T09:15:49.374973Z",
     "shell.execute_reply": "2025-04-12T09:15:49.374542Z",
     "shell.execute_reply.started": "2025-04-12T09:15:49.372235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 罗杰有五个网球，他又买了两盒网球，每盒有3个网球，请问他现在总共有多少个网球？\n",
      "罗杰一开始有五个网球，又购买了两盒网球，每盒3个，共购买了6个网球，因此现在总共由5+6=11个网球。因此答案是11。\n",
      "\n",
      "Question: 食堂总共有23个苹果，如果他们用掉20个苹果，然后又买了6个苹果，请问现在食堂总共有多少个苹果？\n",
      "食堂最初有23个苹果，用掉20个，然后又买了6个，总共有23-20+6=9个苹果，答案是9。\n",
      "\n",
      "Question: 杂耍者可以杂耍16个球。其中一半的球是高尔夫球，其中一半的高尔夫球是蓝色的。请问总共有多少个蓝色高尔夫球？\n",
      "总共有16个球，其中一半是高尔夫球，也就是8个，其中一半是蓝色的，也就是4个，答案是4个。\n",
      "\n",
      "Question: 艾米需要4分钟才能爬到滑梯顶部，她花了1分钟才滑下来，水滑梯将在15分钟后关闭，请问在关闭之前她能滑多少次？\n"
     ]
    }
   ],
   "source": [
    "print(prompt_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83f49313-08c2-4fae-8bd5-d7a730ed7925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:41:55.524206Z",
     "iopub.status.busy": "2025-04-19T18:41:55.523921Z",
     "iopub.status.idle": "2025-04-19T18:41:55.542692Z",
     "shell.execute_reply": "2025-04-19T18:41:55.542172Z",
     "shell.execute_reply.started": "2025-04-19T18:41:55.524191Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DASHSCOPE_API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mDASHSCOPE_API_KEY\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'DASHSCOPE_API_KEY' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be801a22-b281-4392-82ce-a9ccf3c9445d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T09:13:43.741160Z",
     "iopub.status.busy": "2025-04-09T09:13:43.740781Z",
     "iopub.status.idle": "2025-04-09T09:13:44.006551Z",
     "shell.execute_reply": "2025-04-09T09:13:44.005800Z",
     "shell.execute_reply.started": "2025-04-09T09:13:43.741135Z"
    },
    "tags": []
   },
   "source": [
    "## PDF解析 MinerU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad15e23-254d-46b7-bdaa-66b6fe534343",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-11T15:20:27.096485Z",
     "iopub.status.busy": "2025-04-11T15:20:27.096152Z",
     "iopub.status.idle": "2025-04-11T15:20:27.098878Z",
     "shell.execute_reply": "2025-04-11T15:20:27.098471Z",
     "shell.execute_reply.started": "2025-04-11T15:20:27.096461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda create -n mineru 'python>=3.10' -y\n",
    "# conda activate mineru\n",
    "\n",
    "\n",
    "# !pip install -U \"magic-pdf[full]\" -i https://mirrors.aliyun.com/pypi/simple\n",
    "\n",
    "\n",
    "# import os\n",
    "# print(os.getenv(\"DASHSCOPE_API_KEY\"))  # 输出应为 sk-a6ba756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571e79fa-280f-4925-950d-2623f4f8372e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T09:24:32.206006Z",
     "iopub.status.busy": "2025-04-09T09:24:32.205649Z",
     "iopub.status.idle": "2025-04-09T09:24:32.209006Z",
     "shell.execute_reply": "2025-04-09T09:24:32.208417Z",
     "shell.execute_reply.started": "2025-04-09T09:24:32.205983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install modelscope\n",
    "# !wget https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/scripts/download_models.py -O download_models.py\n",
    "# !python download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e327161-e639-455e-9c84-b6c1f42a35a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-11T15:19:18.771313Z",
     "iopub.status.busy": "2025-04-11T15:19:18.771026Z",
     "iopub.status.idle": "2025-04-11T15:19:18.941370Z",
     "shell.execute_reply": "2025-04-11T15:19:18.940903Z",
     "shell.execute_reply.started": "2025-04-11T15:19:18.771296Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 行 1: magic-pdf: 未找到命令\n"
     ]
    }
   ],
   "source": [
    "!magic-pdf -p Day+2：模型I_O与数据连接.pdf  -o minerUOutput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aaee1ee-5139-4ce5-9e5b-28acdb979a44",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-11T15:22:56.940737Z",
     "iopub.status.busy": "2025-04-11T15:22:56.940421Z",
     "iopub.status.idle": "2025-04-11T15:23:35.066639Z",
     "shell.execute_reply": "2025-04-11T15:23:35.066150Z",
     "shell.execute_reply.started": "2025-04-11T15:22:56.940717Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0\n",
      "Uninstalling torch-2.6.0:\n",
      "  Successfully uninstalled torch-2.6.0\n",
      "Found existing installation: torchvision 0.18.1+cpu\n",
      "Uninstalling torchvision-0.18.1+cpu:\n",
      "  Successfully uninstalled torchvision-0.18.1+cpu\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.21.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl (178.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0mm\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.21.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.3.1+cpu\n",
      "    Uninstalling torchaudio-2.3.1+cpu:\n",
      "      Successfully uninstalled torchaudio-2.3.1+cpu\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xtuner 0.1.11 requires bitsandbytes>=0.40.0.post4, which is not installed.\n",
      "xtuner 0.1.11 requires lagent>=0.1.2, which is not installed.\n",
      "xtuner 0.1.11 requires mmengine>=0.10.1, which is not installed.\n",
      "ms-swift 3.2.1 requires transformers<4.51,>=4.33, but you have transformers 4.51.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.6.0+cpu torchaudio-2.6.0+cpu torchvision-0.21.0+cpu\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 更新 torch和torch vision\n",
    "# ==========================\n",
    "\n",
    "# import torch\n",
    "# import torchvision\n",
    "\n",
    "# print(torch.__version__)\n",
    "# print(torchvision.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fdd6c-1a2c-47ac-b34d-41f3e8cd71aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9520f9-8742-4c8b-a917-a030e7f259bc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T08:57:07.301693Z",
     "iopub.status.busy": "2025-04-09T08:57:07.301339Z",
     "iopub.status.idle": "2025-04-09T08:57:07.304892Z",
     "shell.execute_reply": "2025-04-09T08:57:07.304175Z",
     "shell.execute_reply.started": "2025-04-09T08:57:07.301667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# !pip install marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c70d2ee-f2ef-430a-ba55-e2a2e5f57459",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T16:40:16.587929Z",
     "iopub.status.busy": "2025-04-19T16:40:16.587594Z",
     "iopub.status.idle": "2025-04-19T16:40:16.590788Z",
     "shell.execute_reply": "2025-04-19T16:40:16.590351Z",
     "shell.execute_reply.started": "2025-04-19T16:40:16.587911Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 任务1：用OpenAI模型生成提示词模板回复\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# api_key=\"sk-zfkssxfdpozqzvqakjyumrnkllnxewwasqjdkyvswcvwdrat\"\n",
    "# base_url=\"https://api.siliconflow.cn/v1\"\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#    model=\"Pro/deepseek-ai/DeepSeek-R1\",\n",
    "#    openai_api_key=api_key,\n",
    "#    openai_api_base = base_url,\n",
    "#    temperature=0,\n",
    "#    max_tokens=8800\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(\"你是一个{role}，请用{style}风格回答：{query}\")\n",
    "# chain = prompt | llm | StrOutputParser()\n",
    "# print(chain.invoke({\"role\": \"律师\", \"style\": \"严谨\", \"query\": \"如何理解合同法第52条？\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c292d7-5218-420a-b8bf-8ad699b562df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e7a608-ad40-466d-816d-df5f147440ea",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T09:20:44.350828Z",
     "iopub.status.busy": "2025-03-30T09:20:44.350538Z",
     "iopub.status.idle": "2025-03-30T09:20:46.392907Z",
     "shell.execute_reply": "2025-03-30T09:20:46.392466Z",
     "shell.execute_reply.started": "2025-03-30T09:20:44.350799Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是Qwen，一个由阿里云开发的大型语言模型。我的设计目的是通过对话来提供信息、解答问题和进行交流。有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "# export DASHSCOPE_API_KEY='sk-a6ba756d766e41b3afd31c57392b7575'\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "\n",
    "llm = Tongyi(\n",
    "    model=\"qwen-max\",\n",
    ")\n",
    "\n",
    "result = llm.invoke(\"你是谁\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b866ec6-ddaf-4a95-a0aa-fe233c7a1cbd",
   "metadata": {},
   "source": [
    "### 流式响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "623b0b7e-2f4f-4648-a1b4-7ab36c693ff1",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T06:41:50.304070Z",
     "iopub.status.busy": "2025-03-27T06:41:50.303711Z",
     "iopub.status.idle": "2025-03-27T06:41:53.428035Z",
     "shell.execute_reply": "2025-03-27T06:41:53.427555Z",
     "shell.execute_reply.started": "2025-03-27T06:41:50.304048Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是通义千问，阿里巴巴集团旗下的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，欢迎随时告诉我！"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Tongyi  \n",
    "  \n",
    "llm = Tongyi()  \n",
    "for chunk in llm.stream(\"你是谁\"):  \n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb66e72-f30b-49b8-9028-4b83ab6faf26",
   "metadata": {},
   "source": [
    "## batch批量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aff1d496-336c-4f1e-bf79-3c26f4a76c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T06:42:35.123949Z",
     "iopub.status.busy": "2025-03-27T06:42:35.123615Z",
     "iopub.status.idle": "2025-03-27T06:42:48.769921Z",
     "shell.execute_reply": "2025-03-27T06:42:48.769435Z",
     "shell.execute_reply.started": "2025-03-27T06:42:35.123929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我是通义千问，阿里巴巴集团旗下的超大规模语言模型。我能够回答问题、创作文字，比如写故事、公文、邮件、剧本等，还能进行逻辑推理、编程，甚至表达观点和玩游戏。我在多国语言上都有很好的掌握，能为你提供多样化的帮助。有什么我可以帮到你的吗？', '大模型通常指的是参数量非常大的机器学习模型，尤其是在深度学习领域中。这些模型通常具有数十亿甚至上万亿的参数，并且在大规模数据集上进行训练。大模型的主要特点和优势包括：\\n\\n1. **强大的表达能力**：由于参数量巨大，大模型能够捕捉到数据中的复杂模式和细微特征，这使得它们在处理自然语言处理（NLP）、计算机视觉（CV）、语音识别等任务时表现出色。\\n\\n2. **广泛的泛化能力**：大模型经过大量数据的训练后，往往展现出较强的泛化能力，可以在未见过的数据或新场景下依然保持较好的性能。\\n\\n3. **零样本、少样本学习能力**：一些先进的大模型具备很强的零样本（zero-shot）或少样本（few-shot）学习能力，即无需额外训练就能完成某些任务，或者仅需少量示例即可快速适应新任务。\\n\\n4. **知识存储与生成**：大模型内部存储了大量训练过程中学到的知识，因此可以用于回答问题、生成文章、编写代码等多种创造性和推理性任务。\\n\\n然而，大模型也面临着一些挑战，例如计算资源需求高、训练成本昂贵、潜在的安全性和隐私问题以及对环境的影响等。因此，在实际应用中需要综合考虑效果与效率之间的平衡。']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Tongyi  \n",
    "  \n",
    "llm = Tongyi()  \n",
    "result = llm.batch([\"你是谁\", \"什么是大模型\"])  \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e67322-7771-41fe-9fdb-377ed23c4063",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44937d4-0138-4291-8882-a190a237b9c9",
   "metadata": {},
   "source": [
    "### ainvoke异步请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8e98763-a4e7-4ef3-bea3-11b1c5151aca",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T06:43:41.879998Z",
     "iopub.status.busy": "2025-03-27T06:43:41.879619Z",
     "iopub.status.idle": "2025-03-27T06:43:41.903912Z",
     "shell.execute_reply": "2025-03-27T06:43:41.903307Z",
     "shell.execute_reply.started": "2025-03-27T06:43:41.879967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是谁\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)  \n\u001b[0;32m---> 13\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mainvoke_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio  \n",
    "  \n",
    "from langchain_community.llms import Tongyi  \n",
    "\n",
    "llm = Tongyi()  \n",
    "  \n",
    "  \n",
    "async def ainvoke_llm():  \n",
    "    result = await llm.ainvoke(\"你是谁\")  \n",
    "    print(result)  \n",
    "  \n",
    "  \n",
    "asyncio.run(ainvoke_llm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87c9e8cd-f6dc-48a1-a5dc-f3b27efa0011",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T06:54:47.084620Z",
     "iopub.status.busy": "2025-03-27T06:54:47.084295Z",
     "iopub.status.idle": "2025-03-27T06:54:47.608211Z",
     "shell.execute_reply": "2025-03-27T06:54:47.607752Z",
     "shell.execute_reply.started": "2025-03-27T06:54:47.084600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat resp: content='Hello' additional_kwargs={} response_metadata={} id='run-5521ec62-afc7-49d9-b765-69dde418f820'chat resp: content='!' additional_kwargs={} response_metadata={} id='run-5521ec62-afc7-49d9-b765-69dde418f820'chat resp: content=' How' additional_kwargs={} response_metadata={} id='run-5521ec62-afc7-49d9-b765-69dde418f820'chat resp: content=' can I assist you' additional_kwargs={} response_metadata={} id='run-5521ec62-afc7-49d9-b765-69dde418f820'chat resp: content=' today?' additional_kwargs={} response_metadata={} id='run-5521ec62-afc7-49d9-b765-69dde418f820'chat resp: content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'request_id': '8889ff95-b698-96f3-b23a-74fd98b4003e', 'token_usage': {'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'prompt_tokens_details': {'cached_tokens': 0}}} id='run-5521ec62-afc7-49d9-b765-69dde418f820'"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chatLLM = ChatTongyi(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chatLLM.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "for r in res:\n",
    "    print(\"chat resp:\", r, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6e3ecd2-8641-4110-9227-6b1a943a477b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:42:45.523550Z",
     "iopub.status.busy": "2025-04-19T18:42:45.523254Z",
     "iopub.status.idle": "2025-04-19T18:42:48.342708Z",
     "shell.execute_reply": "2025-04-19T18:42:48.342236Z",
     "shell.execute_reply.started": "2025-04-19T18:42:45.523536Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！我是企业内的智能小助手，您可以叫我通义千问。我是由阿里云研发的超大规模语言模型，我的主要职责是为企业员工提供高效、便捷的服务和支持。无论是解答工作中的问题、协助处理日常任务，还是提供信息查询服务，我都可以尽力帮助您。如果您有任何需要，请随时告诉我！\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Tongyi  \n",
    "from langchain_core.messages import SystemMessage, HumanMessage  \n",
    "  \n",
    "message = [SystemMessage(\"你是一个企业内的智能小助手\"), HumanMessage(\"你是谁\")]  \n",
    "llm = Tongyi()  \n",
    "result = llm.invoke(message)  \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a71c9afa-1ed6-4fdc-b04e-77dcaf333f29",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T07:52:48.819036Z",
     "iopub.status.busy": "2025-03-27T07:52:48.818702Z",
     "iopub.status.idle": "2025-03-27T07:52:48.822524Z",
     "shell.execute_reply": "2025-03-27T07:52:48.822096Z",
     "shell.execute_reply.started": "2025-03-27T07:52:48.819015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['product'] input_types={} partial_variables={} template='\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes {product}?\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# 定义一个模板，包含一个变量 {product}\n",
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "result = prompt.format(product=\"don't know\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "710f4cfe-0308-42f3-90ce-df4110c6b8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T06:59:59.264899Z",
     "iopub.status.busy": "2025-03-27T06:59:59.264401Z",
     "iopub.status.idle": "2025-03-27T07:00:11.455485Z",
     "shell.execute_reply": "2025-03-27T07:00:11.455021Z",
     "shell.execute_reply.started": "2025-03-27T06:59:59.264879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！薛定谔的猫是一个由物理学家埃尔温·薛定谔提出的著名思想实验，用来解释量子力学中的一些奇怪概念。以下是一个通俗易懂的解释：\n",
      "\n",
      "---\n",
      "\n",
      "想象一下，有一只猫被关在一个密封的盒子里，盒子里还有一个特殊的装置。这个装置包含一个放射性原子、一个探测器和一瓶毒气。如果放射性原子发生衰变（这是一个随机事件），探测器会检测到它，并释放毒气，导致猫死亡。但如果原子没有衰变，毒气就不会释放，猫就会活着。\n",
      "\n",
      "根据量子力学的规则，在我们打开盒子观察之前，放射性原子的状态是“既衰变又没有衰变”的叠加状态。因此，猫的状态也是“既活着又死了”的叠加状态。\n",
      "\n",
      "只有当我们打开盒子观察时，这种叠加状态才会“坍缩”成一种确定的结果：要么猫活着，要么猫死了。\n",
      "\n",
      "---\n",
      "\n",
      "### 这个实验的目的是什么？\n",
      "薛定谔设计这个思想实验并不是为了真的把猫关进盒子里，而是为了说明量子力学中的一个核心问题：**在微观世界中，粒子可以处于叠加状态，但我们在宏观世界中却从未见过类似的现象（比如一只既生又死的猫）。**\n",
      "\n",
      "通过这个实验，薛定谔试图引发人们对量子力学诠释的思考：为什么量子世界的规则看起来与我们的日常经验如此不同？\n",
      "\n",
      "---\n",
      "\n",
      "简单来说，薛定谔的猫告诉我们：在量子力学中，直到你去观察，某些事情才会有明确的结果。而在观察之前，它们可能同时处于多种可能性之中！\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.tongyi import Tongyi  \n",
    "from langchain_core.prompts import PromptTemplate  \n",
    "  \n",
    "prompt_template = PromptTemplate.from_template(\"请给我一个关于{topic}的{type}解释。\")  \n",
    "prompt = prompt_template.format(type=\"通俗易懂的\", topic=\"薛定谔的猫\")  \n",
    "llm = Tongyi()  \n",
    "result = llm.invoke(prompt)  \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84b707f6-d0c0-4b57-b986-13131b034029",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T08:00:37.862856Z",
     "iopub.status.busy": "2025-03-27T08:00:37.862520Z",
     "iopub.status.idle": "2025-03-27T08:00:37.868100Z",
     "shell.execute_reply": "2025-03-27T08:00:37.867631Z",
     "shell.execute_reply.started": "2025-03-27T08:00:37.862831Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14496/3823640160.py:4: DeprecationWarning: `input_variables' is deprecated and ignored.\n",
      "  prompt = PromptTemplate.from_file(\"template.txt\", input_variables=[\"name\", \"place\"])\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# 使用 from_file 方法加载模板文件，注意这里指定了输入变量列表\n",
    "prompt = PromptTemplate.from_file(\"template.txt\", input_variables=[\"name\", \"place\"])\n",
    "\n",
    "# 使用 format 方法传入变量，生成最终提示\n",
    "formatted_prompt = prompt.format(name=\"Alice\", place=\"Wonderland\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64a91a67-efc6-48cf-b45a-39874bfb7191",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T08:12:23.669650Z",
     "iopub.status.busy": "2025-03-27T08:12:23.669324Z",
     "iopub.status.idle": "2025-03-27T08:12:25.275669Z",
     "shell.execute_reply": "2025-03-27T08:12:25.275206Z",
     "shell.execute_reply.started": "2025-03-27T08:12:23.669630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-04-24 13:35:00\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser  \n",
    "from langchain_community.llms.tongyi import Tongyi  \n",
    "  \n",
    "from langchain_core.prompts import PromptTemplate  \n",
    "  \n",
    "output_parser = DatetimeOutputParser()  \n",
    "\n",
    "template = \"\"\"用户发起的提问:  \n",
    "{question}  \n",
    "{format_instructions}\"\"\"  \n",
    "  \n",
    "prompt = PromptTemplate.from_template(template,  # 预定义的变量，这里我们传入格式化指令  \n",
    "                                      partial_variables={  \n",
    "                                          \"format_instructions\": output_parser.get_format_instructions()})  \n",
    "  \n",
    "chain = prompt | Tongyi() | output_parser  \n",
    "  \n",
    "output = chain.invoke({\"question\": \"长征一号是哪天发射的\"})  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "038f63af-74da-403c-ae1c-8e8a4896b13e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T08:14:07.144279Z",
     "iopub.status.busy": "2025-03-27T08:14:07.143945Z",
     "iopub.status.idle": "2025-03-27T08:14:07.147584Z",
     "shell.execute_reply": "2025-03-27T08:14:07.147159Z",
     "shell.execute_reply.started": "2025-03-27T08:14:07.144258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 992-09-12T20:36:15.659454Z, 304-06-20T04:57:22.972822Z, 901-07-19T14:30:01.806438Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt.format(question = 'nice')\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c73850-ec5b-425b-9eab-78a046d69698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54d9d2e7-3fdd-4037-8a93-79a52a42def8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:58:36.538538Z",
     "iopub.status.busy": "2025-04-19T18:58:36.538238Z",
     "iopub.status.idle": "2025-04-19T18:58:36.541729Z",
     "shell.execute_reply": "2025-04-19T18:58:36.541355Z",
     "shell.execute_reply.started": "2025-04-19T18:58:36.538524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\youwenbin\\AppData\\Local\\Temp\\ipykernel_39836\\3438750752.py:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "  contents = \"\"\"You will analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error.\n"
     ]
    }
   ],
   "source": [
    "contents = \"\"\"You will analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error.\n",
    "Your goal is to explain precisely how their misconception caused them to arrive at the wrong answer.\n",
    "\n",
    "Here is the problem information:\n",
    "<problem_data>\n",
    "# Question: Simplify the expression: \\[x \\cdot y \\cdot x\\]\n",
    "# Correct Answer: \\(x^2y\\)\n",
    "# Incorrect Answer: \\(x^2\\)\n",
    "# Primary Misconception: Ignores variables without explicit coefficients when multiplying\n",
    "</problem_data>\n",
    "\n",
    "Here are related misconceptions that are similar but do not explain this specific error as precisely:\n",
    "<related_misconceptions>\n",
    "- Thinks only like terms can be multiplied\n",
    "- Fails to combine all instances of the same variable\n",
    "- Incorrectly identifies an incomplete variable factor\n",
    "- Does not understand how to multiply algebraic terms\n",
    "</related_misconceptions>\n",
    "\n",
    "First, examine all components of the problem carefully:\n",
    "1. The problem statement and question asked\n",
    "2. The correct answer and solution method\n",
    "3. The student's incorrect answer\n",
    "4. The primary misconception given\n",
    "5. The related misconceptions that should be distinguished from the primary one\n",
    "\n",
    "Then, reconstruct the student's likely thought process:\n",
    "- Identify the exact point where their reasoning diverged from the correct solution path\n",
    "- Note which specific mathematical operations or concepts they misapplied\n",
    "- Connect their error directly to the stated primary misconception\n",
    "- Verify that this explanation better fits the error than the related misconceptions\n",
    "\n",
    "Write your analysis in <evaluation> tags, following this structure:\n",
    "- Show the correct calculation first\n",
    "- Show the incorrect calculations that demonstrate the error\n",
    "- Explain the specific flaw in the student's reasoning\n",
    "- Demonstrate how the misconception led to this particular error\n",
    "- Distinguish from the related misconceptions\n",
    "- Keep your explanation to 5-6 clear, non-repetitive sentences\n",
    "- Focus solely on the reasoning that produced this specific error\n",
    "- keep you answer as simplify as you can, you should responese within 512 tokens\n",
    "\n",
    "Guidelines for writing your explanation:\n",
    "- Do not restate the problem or name the misconception\n",
    "- Be precise about the mathematical concepts involved\n",
    "- Show exactly how the misconception led to the error\n",
    "- Distinguish from related misconceptions\n",
    "- Avoid repetition\n",
    "- Stay focused on this specific error\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d40176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0ed2d55-d610-4245-b719-fd06842b25e5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T16:41:30.163856Z",
     "iopub.status.busy": "2025-04-19T16:41:30.163572Z",
     "iopub.status.idle": "2025-04-19T16:41:53.555748Z",
     "shell.execute_reply": "2025-04-19T16:41:53.555356Z",
     "shell.execute_reply.started": "2025-04-19T16:41:30.163829Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\functools.py:35: RuntimeWarning: coroutine 'test_async_call' was never awaited\n",
      "  def update_wrapper(wrapper,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "思考过程：\n",
      "Okay, let's see. The problem is to simplify x multiplied by y multiplied by x. The correct answer is x squared y. But the student answered x squared.\n",
      "\n",
      "First, the correct approach: When you multiply variables, you combine like terms by adding their exponents. Here, x is multiplied twice, so x * x becomes x². The y doesn't have another y to combine with, so it stays as y. So the correct simplified form is x²y.\n",
      "\n",
      "The student's answer is missing the y. Why would they leave out the y? The primary misconception is that they ignore variables without explicit coefficients. Maybe they think only variables with coefficients (like numbers in front) are important. So when they see x * y * x, they focus on the x's because they have coefficients (even though it's implicit 1), and ignore the y because it doesn't have a coefficient written. They multiply the x's to get x² and then disregard the y entirely, leading to x².\n",
      "\n",
      "Looking at the related misconceptions: It's not about combining like terms, because y isn't a like term here. The student didn't fail to combine all instances of x; they did combine x*x. The error isn't about not knowing how to multiply terms, because they handled the x's correctly. The key is specifically ignoring the y because it lacks a coefficient, which isn't covered by the other misconceptions. That makes the primary misconception the best fit here.\n",
      "最终答案：\n",
      "<evaluation>\n",
      "The correct simplification groups like variables: \\(x \\cdot y \\cdot x = x \\cdot x \\cdot y = x^2y\\). \n",
      "\n",
      "The student likely calculated \\(x \\cdot x = x^2\\) but omitted \\(y\\) entirely. \n",
      "\n",
      "This occurs because they treated \\(y\\) as negligible due to its lack of an explicit numerical coefficient (e.g., interpreting it as \\(1y\\) but then dismissing the \"1\" and the variable). \n",
      "\n",
      "Unlike errors involving combining like terms or misapplying exponents, this mistake specifically stems from assuming variables without visible coefficients do not require inclusion in the final product. \n",
      "\n",
      "Related misconceptions like \"fails to combine all instances of the same variable\" do not apply here, as \\(y\\) is unique in the expression and wasn’t overlooked due to repetition.\n",
      "</evaluation>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-r1\",  # 此处以 deepseek-r1 为例，可按需更换模型名称。\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': contents}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 通过reasoning_content字段打印思考过程\n",
    "print(\"思考过程：\")\n",
    "print(completion.choices[0].message.reasoning_content)\n",
    "\n",
    "# 通过content字段打印最终答案\n",
    "print(\"最终答案：\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6767819",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.error'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merror\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Timeout \u001b[38;5;28;01mas\u001b[39;00m OpenAITimeout, APIError\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai.error'"
     ]
    }
   ],
   "source": [
    "from openai.error import Timeout as OpenAITimeout, APIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d069a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da ='s<sss>s'\n",
    "da.__contains__('<sss>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf413a35-c352-4766-a8d4-4a34cac32d91",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T17:56:50.997338Z",
     "iopub.status.busy": "2025-04-19T17:56:50.997047Z",
     "iopub.status.idle": "2025-04-19T17:56:51.000653Z",
     "shell.execute_reply": "2025-04-19T17:56:51.000269Z",
     "shell.execute_reply.started": "2025-04-19T17:56:50.997316Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deepseek_completions(content):\n",
    "    client = OpenAI(\n",
    "        # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-r1\",  # 此处以 deepseek-r1 为例，可按需更换模型名称。\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': content}\n",
    "        ],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-4\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"讲个猫的故事\"}],\n",
    "#     temperature=1.0,\n",
    "#     n=3  # 一次生成多个\n",
    "# )\n",
    "\n",
    "# for i, choice in enumerate(response['choices']):\n",
    "#     print(f\"版本 {i+1}:\\n{choice['message']['content']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511a7c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m异步调用失败！错误类型: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 详情: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_async_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from openai import AsyncOpenAI\n",
    "# import asyncio\n",
    "\n",
    "# async def test_async_call():\n",
    "#     try:\n",
    "#         client = AsyncOpenAI(\n",
    "#             api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "#             base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "#         )\n",
    "        \n",
    "#         # 最小测试请求（缩短内容）\n",
    "#         response = await client.chat.completions.create(\n",
    "#             model=\"deepseek-r1\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": \"只说 'PING' 不要其他内容\"}],\n",
    "#             temperature=0,\n",
    "#             timeout=10  # 缩短超时时间\n",
    "#         )\n",
    "#         print(\"异步测试成功！响应内容:\", response.choices[0].message.content)\n",
    "#     except Exception as e:\n",
    "#         print(f\"异步调用失败！错误类型: {type(e).__name__}, 详情: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(test_async_call())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "661aa5d8-384c-4c68-80b5-13f6e022d075",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T17:59:34.770113Z",
     "iopub.status.busy": "2025-04-19T17:59:34.769831Z",
     "iopub.status.idle": "2025-04-19T18:03:27.013182Z",
     "shell.execute_reply": "2025-04-19T18:03:27.012765Z",
     "shell.execute_reply.started": "2025-04-19T17:59:34.770097Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "correct_answer = []\n",
    "for i,zhipu_res in enumerate(res):\n",
    "    translate = f\"\"\"Please help me rewrite the following text if that contains mixed Chinese and English. Your goal is to:\n",
    "\n",
    "1. Detect and correct any Chinese parts into fluent English.\n",
    "2. Preserve the original meaning as accurately as possible.\n",
    "3. Provide two different English rewritings to show diversity in phrasing or tone.\n",
    "\n",
    "Text:\n",
    "\"{zhipu_res}\"\n",
    "\n",
    "Output:\n",
    "Version 1: [Your rewritten version here]\n",
    "\n",
    "Version 2: [Your alternative version here]\n",
    "\"\"\"\n",
    "    correct_answer.append( get_deepseek_completions( translate ) )\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98e275d3-f899-42b3-b419-59ac92f13718",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:05:28.153852Z",
     "iopub.status.busy": "2025-04-19T18:05:28.153564Z",
     "iopub.status.idle": "2025-04-19T18:05:28.156986Z",
     "shell.execute_reply": "2025-04-19T18:05:28.156648Z",
     "shell.execute_reply.started": "2025-04-19T18:05:28.153837Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Output:  \\nVersion 1:  \\nThe correct computation of \\\\(x \\\\cdot y \\\\cdot x\\\\) requires multiplying all terms sequentially: \\\\(x \\\\cdot x = x^2\\\\), then multiplying by \\\\(y\\\\) to yield \\\\(x^2y\\\\). The student’s error likely stemmed from disregarding the \\\\(y\\\\) term entirely, simplifying the expression as \\\\(x \\\\cdot y \\\\cdot x = x \\\\cdot x = x^2\\\\). The critical flaw here is the failure to acknowledge \\\\(y\\\\) during multiplication, treating it as inconsequential or nonexistent. This reflects a specific misunderstanding of variable inclusion—unlike errors involving combining like terms or general algebraic multiplication rules, the student’s mistake was uniquely tied to overlooking variables without numerical coefficients. By neglecting \\\\(y\\\\), the final result erroneously became \\\\(x^2\\\\) instead of \\\\(x^2y\\\\).  \\n\\nVersion 2:  \\nTo compute \\\\(x \\\\cdot y \\\\cdot x\\\\) accurately, one must multiply all components: first \\\\(x \\\\cdot x = x^2\\\\), then multiply by \\\\(y\\\\) to obtain \\\\(x^2y\\\\). The student’s incorrect approach appears to have omitted \\\\(y\\\\) entirely, reducing the expression to \\\\(x \\\\cdot x = x^2\\\\). The core issue lies in their exclusion of \\\\(y\\\\) during the multiplication process, likely due to perceiving it as insignificant or unnecessary. This error highlights a distinct misconception—ignoring variables that lack explicit coefficients—rather than broader issues like misapplying multiplication rules or combining terms improperly. Consequently, the absence of \\\\(y\\\\) in their final answer (\\\\(x^2\\\\) instead of \\\\(x^2y\\\\)) directly stems from this oversight.  \\n\\n---  \\n**Note**: The original text contained no Chinese, so the rewrites focus on refining phrasing, tone, and structural clarity while preserving the analysis. Version 1 adopts a slightly formal tone, while Version 2 simplifies syntax for readability.',\n",
       " 'Output:  \\n**Version 1:**  \\nThe correct calculation is \\\\(x \\\\cdot y \\\\cdot x = x^2 \\\\cdot y\\\\), where the two \\\\(x\\\\) terms multiply to form \\\\(x^2\\\\), which is then multiplied by \\\\(y\\\\). The student’s erroneous result \\\\(x \\\\cdot y \\\\cdot x = x^2\\\\) entirely omits the \\\\(y\\\\) term. The key error lies in the student’s failure to acknowledge that variables without explicit coefficients remain integral factors in the expression. This oversight likely stems from disregarding \\\\(y\\\\) during multiplication, leading to the incorrect simplification to \\\\(x^2\\\\) instead of \\\\(x^2y\\\\). This mistake differs from other common misunderstandings, such as assuming only like terms can be multiplied. Here, the student did not *incorrectly combine terms* but rather *overlooked the presence of the y variable entirely*. It also contrasts with errors where learners fail to consolidate repeated variables, as the student correctly combined the \\\\(x\\\\) terms but excluded \\\\(y\\\\).  \\n\\n**Version 2:**  \\nIn the correct solution \\\\(x \\\\cdot y \\\\cdot x = x^2 \\\\cdot y\\\\), multiplying the two \\\\(x\\\\) terms yields \\\\(x^2\\\\), which is then scaled by \\\\(y\\\\). The student’s flawed answer \\\\(x \\\\cdot y \\\\cdot x = x^2\\\\) mistakenly eliminates the \\\\(y\\\\) factor. The core issue is the student’s assumption that variables lacking numerical coefficients can be dismissed during multiplication—a misconception causing them to discard \\\\(y\\\\) and erroneously conclude \\\\(x^2\\\\). Unlike errors involving *erroneously combining unlike terms*, this mistake reflects a total *neglect of the y variable*. It also diverges from errors where learners incompletely merge identical variables, as the student properly combined the \\\\(x\\\\) terms but omitted \\\\(y\\\\) altogether.',\n",
       " 'Version 1:  \\n<evaluation>  \\nThe accurate computation should be: \\\\[ x \\\\cdot y \\\\cdot x = x^2 \\\\cdot y \\\\], resulting in \\\\(x^2y\\\\).  \\n\\nThe student\\'s erroneous solution stated: \\\\[ x \\\\cdot y \\\\cdot x = x^2 \\\\], disregarding the \\\\(y\\\\) term.  \\n\\nThe core issue in the student\\'s logic lies in their failure to acknowledge that all variables—even those lacking visible coefficients—must remain in the final expression. This suggests they selectively excluded variables without explicit numerical multipliers during multiplication.  \\n\\nThis oversight caused the student to eliminate \\\\(y\\\\) completely, yielding \\\\(x^2\\\\) rather than \\\\(x^2y\\\\). The mistake originates from an incorrect assumption that variables without coefficients need not be retained during multiplication.  \\n\\nThis error differs from similar algebraic misunderstandings because the student demonstrates competency in combining like terms and recognizing variable factors. Instead, the issue is narrowly confined to dismissing a coefficient-free variable, distinguishing it from broader errors like improper term multiplication or incomplete factor analysis.  \\n</evaluation>  \\n\\nVersion 2:  \\n<evaluation>  \\nProper solution: \\\\[ x \\\\cdot y \\\\cdot x = x^2 \\\\cdot y \\\\], simplifying to \\\\(x^2y\\\\).  \\n\\nThe student incorrectly derived: \\\\[ x \\\\cdot y \\\\cdot x = x^2 \\\\], omitting the \\\\(y\\\\) variable entirely.  \\n\\nThe critical flaw here is the student’s assumption that variables lacking explicit coefficients (like \\\\(y\\\\)) can be discarded during multiplication. This reflects a selective exclusion of variables based on coefficient visibility rather than algebraic rules.  \\n\\nBy neglecting \\\\(y\\\\), the student erroneously reduced the expression to \\\\(x^2\\\\). The root cause is a misinterpretation of multiplicative inclusivity—all variables, regardless of coefficient presence, must be preserved.  \\n\\nUnlike confusion with combining like terms or misapplying exponent rules, this error uniquely centers on dismissing \"bare\" variables. It highlights a specific gap in understanding multiplicative completeness rather than general algebraic mechanics.  \\n</evaluation>',\n",
       " 'Output:  \\n**Version 1**:  \\n<evaluation>  \\n**Correct calculation**: \\\\(x \\\\cdot y \\\\cdot x = x^1 \\\\cdot y^1 \\\\cdot x^1 = x^{1+1} \\\\cdot y = x^2y\\\\).  \\n\\n**Incorrect calculation**: \\\\(x \\\\cdot y \\\\cdot x = x \\\\cdot x = x^2\\\\).  \\n\\nThe student correctly recognized that \\\\(x \\\\cdot x = x^2\\\\) but neglected to include the \\\\(y\\\\) term in their final result. This indicates that the student **overlooked the existence of the variable \\\\(y\\\\)**, likely due to assuming that only variables with explicit coefficients (e.g., \\\\(2x\\\\) or \\\\(3y\\\\)) require inclusion in multiplication. This misunderstanding led them to discard \\\\(y\\\\) entirely, yielding \\\\(x^2\\\\) instead of \\\\(x^2y\\\\).  \\n\\nThis error differs from conflating multiplication with combining like terms, as the student did multiply \\\\(x \\\\cdot x\\\\) correctly. It also cannot be attributed to failing to consolidate identical variables, since \\\\(x\\\\) terms were properly combined. The mistake does not stem from misidentifying incomplete variable factors or a general confusion about algebraic multiplication rules, as exponentiation was applied correctly to \\\\(x\\\\).  \\n\\nThe specific flaw lies in the student’s assumption that a variable lacking an explicit coefficient (in this case, \\\\(y\\\\)) is irrelevant to the multiplication process, directly causing \\\\(y\\\\)’s omission.  \\n</evaluation>  \\n\\n**Version 2**:  \\n<evaluation>  \\n**Accurate solution**: \\\\(x \\\\cdot y \\\\cdot x = x^1 \\\\cdot y^1 \\\\cdot x^1 = x^{1+1} \\\\cdot y = x^2y\\\\).  \\n\\n**Erroneous solution**: \\\\(x \\\\cdot y \\\\cdot x = x \\\\cdot x = x^2\\\\).  \\n\\nWhile the student properly simplified \\\\(x \\\\cdot x\\\\) to \\\\(x^2\\\\), they **failed to account for the \\\\(y\\\\) term** in their answer. This suggests the student mistakenly believed that variables without numerical coefficients (such as \\\\(y\\\\) here) can be disregarded during multiplication, resulting in the incomplete expression \\\\(x^2\\\\).  \\n\\nThis error is unique. Unlike cases where students confuse multiplication with combining like terms, here the student multiplied like terms (\\\\(x \\\\cdot x\\\\)) appropriately. It also does not reflect a failure to unify repeated variables, as \\\\(x\\\\) was consolidated correctly. The issue is not rooted in misapplying exponent rules or misjudging variable factors but rather in the student’s **unfounded exclusion of \\\\(y\\\\) due to its lack of a coefficient**.  \\n\\nThe root cause stems from the student’s implicit assumption that coefficients determine a variable’s relevance in multiplication, leading them to omit \\\\(y\\\\) entirely.  \\n</evaluation>  \\n\\n---  \\n**Key differences**:  \\n- **Tone**: Version 1 uses phrases like “overlooked the existence” and “discard,” while Version 2 opts for “failed to account for” and “disregarded.”  \\n- **Structure**: Version 1 emphasizes the distinctness of the error from other misconceptions, whereas Version 2 highlights the uniqueness of the mistake through comparative framing.  \\n- **Phrasing**: Version 2 uses “root cause” and “unfounded exclusion” for analytical clarity, while Version 1 focuses on “assumption” and “irrelevance” to underscore the logic gap.',\n",
       " 'Version 1:  \\n**Evaluation**  \\nThe correct calculation for \\\\(x \\\\cdot y \\\\cdot x\\\\) involves multiplying all terms: first \\\\(x \\\\cdot x = x^2\\\\), then multiplying by \\\\(y\\\\) to obtain \\\\(x^2y\\\\).  \\n\\nThe student’s incorrect result \\\\(x \\\\cdot y \\\\cdot x = x^2\\\\) neglected to include the \\\\(y\\\\) term in the final product.  \\n\\nThe specific flaw in the student’s reasoning lies in their oversight of the variable \\\\(y\\\\). They may have assumed that variables without explicit coefficients or exponents can be disregarded during multiplication.  \\n\\nThis misconception caused the error because the student focused solely on consolidating the powers of \\\\(x\\\\) and overlooked the necessity of retaining \\\\(y\\\\) in the expression.  \\n\\nThis differs from related errors, such as believing only like terms can be multiplied, as the student did not dismiss \\\\(y\\\\) entirely but failed to carry it forward. It also contrasts with errors in combining variables, as the student correctly handled \\\\(x \\\\cdot x\\\\) but omitted \\\\(y\\\\).  \\n\\n---\\n\\nVersion 2:  \\n**Analysis**  \\nTo compute \\\\(x \\\\cdot y \\\\cdot x\\\\) accurately, all factors must be multiplied sequentially: \\\\(x \\\\cdot x\\\\) yields \\\\(x^2\\\\), and multiplying this by \\\\(y\\\\) results in \\\\(x^2y\\\\).  \\n\\nThe student erroneously concluded \\\\(x \\\\cdot y \\\\cdot x = x^2\\\\), omitting the \\\\(y\\\\) component entirely.  \\n\\nThe core issue stems from the student’s failure to acknowledge the presence of \\\\(y\\\\). They may have mistakenly treated \\\\(y\\\\) as negligible because it lacked an explicit coefficient or exponent.  \\n\\nThis oversight led to the mistake, as the student prioritized consolidating \\\\(x\\\\)’s terms but ignored the requirement to retain all variables in the product.  \\n\\nUnlike misconceptions about combining like terms or mismanaging identical variables, the error here is unique: the student recognized \\\\(x \\\\cdot x\\\\) but excluded \\\\(y\\\\), suggesting a partial—not total—disregard for variables without coefficients.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e01b0d-e096-42bf-ac34-7f6b3f11faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69bf8dd4-de34-48a5-8687-dfe4875a8368",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T17:41:34.350168Z",
     "iopub.status.busy": "2025-04-19T17:41:34.349862Z",
     "iopub.status.idle": "2025-04-19T17:42:05.359908Z",
     "shell.execute_reply": "2025-04-19T17:42:05.359470Z",
     "shell.execute_reply.started": "2025-04-19T17:41:34.350153Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install zhipuai 请先在终端进行安装\n",
    "\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key=\"8da93f1e7e7dd473acaf8f1d04964d50.AUqRClXydos08HBQ\")\n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in mathematical error analysis. Your task is to analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error. And remember always answer in English language.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": contents\n",
    "            }\n",
    "        ],\n",
    "        top_p= 0.9,\n",
    "        temperature= 0.95,\n",
    "        max_tokens=1024,\n",
    "        tools = [{\"type\":\"web_search\",\"web_search\":{\"search_result\":False}}],\n",
    "        stream=False,\n",
    "        do_sample=True\n",
    "    )\n",
    "    res.append(response.choices[0].message.content)\n",
    "    \n",
    "    # messages=[\n",
    "    #     {'role': 'user', 'content': contents}\n",
    "    # ],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1424028f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RequestRate' from 'pyrate_limiter' (d:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pyrate_limiter\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncOpenAI\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtenacity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyrate_limiter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Duration, RequestRate, Limiter\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprometheus_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m start_http_server, Counter, Histogram\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'RequestRate' from 'pyrate_limiter' (d:\\COMPETITIONS\\LangchainExam\\venv\\Lib\\site-packages\\pyrate_limiter\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from pyrate_limiter import Duration, RequestRate, Limiter\n",
    "from prometheus_client import start_http_server, Counter, Histogram\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5140f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'd:\\\\COMPETITIONS\\\\LLMNOTES\\\\where': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall pyrate_limiter -y\n",
    "# !pip install pyrate-limiter==2.10 \n",
    "# !pip show pyrate-limiter\n",
    "# !python where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2997a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = \"D:/COMPETITIONS/LLMNOTES\"\n",
    "\n",
    "# list(Config.OUTPUT_DIR.glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a752c594",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m P = Path(\u001b[33m\"\u001b[39m\u001b[33mD:/COMPETITIONS/LLMNOTES\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# P.mkdir(exist_ok=True, parents=True)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m [\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m os.listdir(P)]\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "P = Path(\"D:/COMPETITIONS/LLMNOTES\")\n",
    "# P.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "[c.name for c in os.listdir(P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17008515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevP = Path(\"../LightgbmSource\")\n",
    "prevP.exists()  # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851e7c5d-78d5-455c-b53f-0ea5c4ce90f7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T17:42:08.032478Z",
     "iopub.status.busy": "2025-04-19T17:42:08.032189Z",
     "iopub.status.idle": "2025-04-19T17:42:08.035598Z",
     "shell.execute_reply": "2025-04-19T17:42:08.035175Z",
     "shell.execute_reply.started": "2025-04-19T17:42:08.032463Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\youwenbin\\AppData\\Local\\Temp\\ipykernel_39836\\3694463574.py:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  df = pd.read_csv(\"D:\\COMPETITIONS\\LLMNOTES\\synthetic.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1604.000000\n",
       "mean        1.854738\n",
       "std         0.874949\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max         3.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:\\COMPETITIONS\\LLMNOTES\\synthetic.csv\")\n",
    "df['content_id'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "150ed8ad-5872-49d2-b82d-c562ed501933",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:18:24.801624Z",
     "iopub.status.busy": "2025-04-19T18:18:24.801301Z",
     "iopub.status.idle": "2025-04-19T18:18:38.961202Z",
     "shell.execute_reply": "2025-04-19T18:18:38.960813Z",
     "shell.execute_reply.started": "2025-04-19T18:18:24.801609Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To analyze the misconception underlying answer choice (a) 24, let's first understand the correct reasoning and then identify where the student's thought process likely went wrong.\\n\\n**Correct Calculation:**\\nFor each of the 4 questions, there are 5 possible answer choices. Therefore, the total number of ways to complete the test is calculated by multiplying the number of choices for each question:\\n\\\\[ 5 \\\\times 5 \\\\times 5 \\\\times 5 = 5^4 = 625 \\\\]\\n\\n**Incorrect Calculation Leading to Answer (a) 24:**\\nThe student might have误认为 they needed to use permutations or a different combinatorial approach, perhaps thinking they were arranging a smaller set of choices.\\n\\n**Specific Flaw in Reasoning:**\\nThe student likely误解了问题的本质，可能认为每个问题只能选择一个独特的答案，并且这些答案需要以某种方式排列。这种误解可能导致他们使用了排列公式而不是简单的乘法原理。\\n\\n**How the Misconception Led to This Error:**\\nIf the student thought they were arranging 4 unique choices out of 5 (which is not the case here), they might have used the permutation formula for 5 items taken 4 at a time:\\n\\\\[ P(5, 4) = \\\\frac{5!}{(5-4)!} = 5 \\\\times 4 \\\\times 3 \\\\times 2 = 120 \\\\]\\nHowever, to arrive at 24, they might have further misapplied this concept, perhaps by mistakenly halving 120 or using an incorrect subset of permutations.\\n\\n**Distinguishing from Related Misconceptions:**\\nThis error is not due to a misunderstanding of the multiplication principle (which would lead to 625) nor a simple arithmetic mistake. Instead, it reflects a deeper misconception about the nature of choices and arrangements, possibly mixing up permutation concepts with the straightforward multiplication of independent choices.\\n\\n**<evaluation>**\\nThe correct calculation involves multiplying the number of choices for each question: \\\\(5 \\\\times 5 \\\\times 5 \\\\times 5 = 5^4 = 625\\\\). The student who chose 24 likely misapplied permutation concepts, thinking they needed to arrange a subset of choices rather than simply multiplying the number of options for each question. This led them to an incorrect calculation, possibly involving a flawed permutation formula or a misunderstanding of how to handle multiple choices independently. This error is distinct from a simple multiplication mistake or misunderstanding of the problem's combinatorial nature, as it suggests a specific misapplication of permutation principles.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_zhipu_multi_completions(msges):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"glm-4-plus\",\n",
    "            messages=msges,\n",
    "            top_p= 0.9,\n",
    "            temperature= 0.95,\n",
    "            max_tokens=1024,\n",
    "            tools = [{\"type\":\"web_search\",\"web_search\":{\"search_result\":False}}],\n",
    "            stream=False,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in mathematical error analysis. Your task is to analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error. And remember always answer in English language.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": contents\n",
    "            }\n",
    "        ]\n",
    "new_ques = '''\"Problem\": \"a multiple choice test consists of 4 questions , and each question has 5 answer choices . in how many r ways can the test be completed if every question is unanswered ?\",\n",
    "    \"Rationale\": \"\\\"5 choices for each of the 4 questions , thus total r of 5 * 5 * 5 * 5 = 5 ^ 4 = 625 ways to answer all of them . answer : c .\\\"\",\n",
    "    \"options\": \"a ) 24 , b ) 120 , c ) 625 , d ) 720 , e ) 1024\"\n",
    "\n",
    "what is the misconception under answer a'''\n",
    "\n",
    "messages.append({'role': 'assistant', 'content': correct_answer[0].split(\"Version\")[1].replace(\"1:  \",\"\")},)\n",
    "messages.append({'role': 'user', 'content': new_ques },)\n",
    "get_zhipu_multi_completions(messages)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1bcfbae-350c-423b-b5f6-c8208f28e2b9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:20:07.121129Z",
     "iopub.status.busy": "2025-04-19T18:20:07.120838Z",
     "iopub.status.idle": "2025-04-19T18:20:23.083280Z",
     "shell.execute_reply": "2025-04-19T18:20:23.082908Z",
     "shell.execute_reply.started": "2025-04-19T18:20:07.121115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To analyze the misconception underlying the incorrect answer choice (b) 120, let's first understand the correct reasoning and then identify where the student's thought process likely went wrong.\\n\\n### Correct Reasoning:\\nEach of the 4 questions has 5 answer choices. The total number of ways to complete the test is calculated by multiplying the number of choices for each question:\\n\\\\[ 5 \\\\times 5 \\\\times 5 \\\\times 5 = 5^4 = 625 \\\\]\\n\\n### Incorrect Answer (b) 120:\\nThe student chose 120, which suggests a different calculation or misunderstanding.\\n\\n### Analysis of Misconception:\\nTo arrive at 120, the student might have误用 a permutation or combination formula without correctly applying it to the context. Here are a few possibilities:\\n\\n1. **Misapplication of Factorials**: The student might have used a factorial calculation that doesn't fit the problem, such as \\\\( 5! / 3! \\\\), which equals 120. This would imply they thought they were arranging or selecting a subset of choices, rather than independently choosing an answer for each question.\\n\\n2. **Incorrect Permutation/Combination Concept**: They might have误用 a permutation formula thinking they were selecting a sequence of choices, but the formula they used didn't match the problem's requirements.\\n\\n### Specific Flaw in Reasoning:\\nThe primary misconception here is likely a **misapplication of combinatorial principles**. The student did not correctly understand that each question's answer choice is independent of the others, leading them to use an inappropriate formula or reasoning that resulted in 120.\\n\\n### Distinguishing from Correct Reasoning:\\n- **Correct Approach**: Recognizes the independence of each question's choices, leading to \\\\( 5^4 \\\\).\\n- **Incorrect Approach**: Misapplies combinatorial principles, possibly thinking they are arranging or selecting a subset of choices, leading to 120.\\n\\n### <evaluation>\\nThe correct calculation involves multiplying the number of choices for each independent question: \\\\( 5 \\\\times 5 \\\\times 5 \\\\times 5 = 5^4 = 625 \\\\). The student’s incorrect calculation of 120 suggests a misapplication of combinatorial principles, such as using \\\\( 5! / 3! \\\\) or another inappropriate formula. This error stems from not recognizing the independence of each question's choices, leading to an incorrect arrangement or selection logic. Unlike the correct approach, which treats each choice independently, the student's mistake reflects a misunderstanding of how to apply combinatorial concepts to independent events. This specific flaw distinguishes it from correctly applying multiplication for independent choices.\\n</evaluation>\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in mathematical error analysis. Your task is to analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error. And remember always answer in English language.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": contents\n",
    "            }\n",
    "        ]\n",
    "new_ques = '''\"Problem\": \"a multiple choice test consists of 4 questions , and each question has 5 answer choices . in how many r ways can the test be completed if every question is unanswered ?\",\n",
    "    \"Rationale\": \"\\\"5 choices for each of the 4 questions , thus total r of 5 * 5 * 5 * 5 = 5 ^ 4 = 625 ways to answer all of them . answer : c .\\\"\",\n",
    "    \"options\": \"a ) 24 , b ) 120 , c ) 625 , d ) 720 , e ) 1024\"\n",
    "\n",
    "what is the misconception under answer b.'''\n",
    "\n",
    "messages.append({'role': 'assistant', 'content': correct_answer[0].split(\"Version\")[1].replace(\"1:  \",\"\")},)\n",
    "messages.append({'role': 'user', 'content': new_ques },)\n",
    "get_zhipu_multi_completions(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a461f65d-fe6e-48a7-a1f9-255a2c938a03",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T19:13:36.482994Z",
     "iopub.status.busy": "2025-04-19T19:13:36.482529Z",
     "iopub.status.idle": "2025-04-19T19:14:03.716165Z",
     "shell.execute_reply": "2025-04-19T19:14:03.715744Z",
     "shell.execute_reply.started": "2025-04-19T19:13:36.482980Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<evaluation>\\nCorrect calculation: \\\\(x \\\\cdot y \\\\cdot x = (x \\\\cdot x) \\\\cdot y = x^2y\\\\).  \\nIncorrect calculation: The student computed \\\\(x \\\\cdot x = x^2\\\\) while omitting the \\\\(y\\\\), yielding \\\\(x^2\\\\).  \\n\\nThe flaw: The student treated \\\\(y\\\\) as irrelevant because it lacked an explicit coefficient (\\\\(1y\\\\) vs. just \\\\(y\\\\)). They assumed variables without numbers do not participate in multiplication.  \\n\\nThis misconception directly caused the error by excluding \\\\(y\\\\) entirely instead of retaining it as a factor.  \\n\\nDistinguished from related misconceptions: The student did combine \\\\(x\\\\) terms (so not \"fails to combine instances\" or \"incorrectly identifies factors\") and did multiply variables (so not \"only like terms can be multiplied\"). The error stemmed specifically from disregarding variables with implicit coefficients.  \\n</evaluation>'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url='https://api-inference.modelscope.cn/v1/',\n",
    "#     api_key='2e5accb1-9018-49ad-82d4-b2f25e5914b6', # ModelScope Token\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "def get_Qwen_multi_completions(msges):\n",
    "    client = OpenAI(\n",
    "        # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwq-plus\",  # 此处以 deepseek-r1 为例，可按需更换模型名称。\n",
    "        messages=msges,\n",
    "        temperature=1.0,\n",
    "        stream  = True\n",
    "    )\n",
    "    answer_content = \"\"     # 定义完整回复\n",
    "    is_answering = False   # 判断是否结束思考过程并开始回复\n",
    "    for chunk in completion:\n",
    "        # 如果chunk.choices为空，则打印usage\n",
    "        if not chunk.choices:\n",
    "            answer_content+=\"\"\n",
    "        else:\n",
    "            delta = chunk.choices[0].delta\n",
    "            # 打印思考过程\n",
    "            if hasattr(delta, 'reasoning_content') and delta.reasoning_content != None:\n",
    "                # print(delta.reasoning_content, end='', flush=True)\n",
    "                pass\n",
    "            else:\n",
    "                # 开始回复\n",
    "                if delta.content != \"\" and is_answering is False:\n",
    "                    # print(\"\\n\" + \"=\" * 20 + \"完整回复\" + \"=\" * 20 + \"\\n\")\n",
    "                    is_answering = True\n",
    "                # 打印回复过程\n",
    "                # print(delta.content, end='', flush=True)\n",
    "                answer_content += delta.content\n",
    "    \n",
    "    return answer_content\n",
    "\n",
    "\n",
    "# def get_Qwen_multi_completions(msges):\n",
    "#     client = OpenAI(\n",
    "#         base_url='https://api-inference.modelscope.cn/v1/',\n",
    "#         api_key=\"2e5accb1-9018-49ad-82d4-b2f25e5914b6\"\n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     response = client.chat.completions.create(\n",
    "#         model='Qwen/Qwen2.5-72B-Instruct', # ModelScope Model-Id\n",
    "#         messages=msges,\n",
    "#         stream=False\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in mathematical error analysis. Your task is to analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error. And remember always answer in English language.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": contents\n",
    "            }\n",
    "        ]\n",
    "get_Qwen_multi_completions(messages)\n",
    "\n",
    "# qwen2.5-math-72b-instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f389435-10b3-4ce3-818a-5357923fc7a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:20:37.721985Z",
     "iopub.status.busy": "2025-04-19T19:20:37.721687Z",
     "iopub.status.idle": "2025-04-19T19:20:39.789961Z",
     "shell.execute_reply": "2025-04-19T19:20:39.789575Z",
     "shell.execute_reply.started": "2025-04-19T19:20:37.721970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<evaluation>\\nThe correct simplification is: $x \\\\cdot y \\\\cdot x = x^2y$. The student's incorrect answer was $x^2$, indicating they ignored the variable $y$. This error occurred because the student failed to include $y$ in the product, treating it as if it were not present. This directly relates to ignoring variables without explicit coefficients when multiplying. Unlike failing to combine like terms or misunderstanding how to multiply algebraic terms, this error specifically involves omitting a variable. The student's reasoning flaw was not about combining like terms or understanding multiplication rules, but rather about neglecting to include all variables in the result.\\n</evaluation>\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from http import HTTPStatus\n",
    "import os\n",
    "import dashscope\n",
    "\n",
    "\n",
    "def get_llama_multi_completions(messages):\n",
    "\n",
    "    response = dashscope.MultiModalConversation.call(\n",
    "        # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "        api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "        model='llama-4-maverick-17b-128e-instruct', \n",
    "        messages=messages,\n",
    "    )\n",
    "    if response.status_code == HTTPStatus.OK:\n",
    "        return response.output.choices[0].message.content[0][\"text\"]\n",
    "    else:\n",
    "        return ('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "            response.request_id, response.status_code,\n",
    "            response.code, response.message\n",
    "        ))\n",
    "\n",
    "get_llama_multi_completions(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e6155-3f92-4c90-b606-4a649245aa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cdb84c6-0cd2-4d0d-8e34-2ff4e2513908",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:30.569267Z",
     "iopub.status.busy": "2025-04-19T18:54:30.568988Z",
     "iopub.status.idle": "2025-04-19T18:54:30.571910Z",
     "shell.execute_reply": "2025-04-19T18:54:30.571542Z",
     "shell.execute_reply.started": "2025-04-19T18:54:30.569251Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will analyze a student's incorrect answer to identify the specific reasoning flaw that led to their error.\n",
      "Your goal is to explain precisely how their misconception caused them to arrive at the wrong answer.\n",
      "\n",
      "Here is the problem information:\n",
      "<problem_data>\n",
      "# Question: Simplify the expression: \\[x \\cdot y \\cdot x\\]\n",
      "# Correct Answer: \\(x^2y\\)\n",
      "# Incorrect Answer: \\(x^2\\)\n",
      "# Primary Misconception: Ignores variables without explicit coefficients when multiplying\n",
      "</problem_data>\n",
      "\n",
      "Here are related misconceptions that are similar but do not explain this specific error as precisely:\n",
      "<related_misconceptions>\n",
      "- Thinks only like terms can be multiplied\n",
      "- Fails to combine all instances of the same variable\n",
      "- Incorrectly identifies an incomplete variable factor\n",
      "- Does not understand how to multiply algebraic terms\n",
      "</related_misconceptions>\n",
      "\n",
      "First, examine all components of the problem carefully:\n",
      "1. The problem statement and question asked\n",
      "2. The correct answer and solution method\n",
      "3. The student's incorrect answer\n",
      "4. The primary misconception given\n",
      "5. The related misconceptions that should be distinguished from the primary one\n",
      "\n",
      "Then, reconstruct the student's likely thought process:\n",
      "- Identify the exact point where their reasoning diverged from the correct solution path\n",
      "- Note which specific mathematical operations or concepts they misapplied\n",
      "- Connect their error directly to the stated primary misconception\n",
      "- Verify that this explanation better fits the error than the related misconceptions\n",
      "\n",
      "Write your analysis in <evaluation> tags, following this structure:\n",
      "- Show the correct calculation first\n",
      "- Show the incorrect calculations that demonstrate the error\n",
      "- Explain the specific flaw in the student's reasoning\n",
      "- Demonstrate how the misconception led to this particular error\n",
      "- Distinguish from the related misconceptions\n",
      "- Keep your explanation to 5-6 clear, non-repetitive sentences\n",
      "- Focus solely on the reasoning that produced this specific error\n",
      "\n",
      "Guidelines for writing your explanation:\n",
      "- Do not restate the problem or name the misconception\n",
      "- Be precise about the mathematical concepts involved\n",
      "- Show exactly how the misconception led to the error\n",
      "- Distinguish from related misconceptions\n",
      "- Avoid repetition\n",
      "- Stay focused on this specific error\n"
     ]
    }
   ],
   "source": [
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d629ab0-bd3a-42d8-908c-2ab3c43ec6ce",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:47:03.333967Z",
     "iopub.status.busy": "2025-04-19T18:47:03.333679Z",
     "iopub.status.idle": "2025-04-19T18:47:04.268691Z",
     "shell.execute_reply": "2025-04-19T18:47:04.268287Z",
     "shell.execute_reply.started": "2025-04-19T18:47:03.333952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'errors': {'message': 'Authenticaton failed, please make sure that a valid ModelScope token is supplied.'}, 'request_id': '5c393737-3137-41b1-b470-ca7d7e0467d5'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m client = OpenAI(\n\u001b[32m      4\u001b[39m     base_url=\u001b[33m'\u001b[39m\u001b[33mhttps://api-inference.modelscope.cn/v1/\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     api_key=\u001b[33m'\u001b[39m\u001b[33msk-a6ba756d766e41b3afd31c57392b7575\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m# ModelScope Token\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-72B-Instruct\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ModelScope Model-Id\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYou are a helpful assistant.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m你好\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(chunk.choices[\u001b[32m0\u001b[39m].delta.content, end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'errors': {'message': 'Authenticaton failed, please make sure that a valid ModelScope token is supplied.'}, 'request_id': '5c393737-3137-41b1-b470-ca7d7e0467d5'}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key='2e5accb1-9018-49ad-82d4-b2f25e5914b6', # ModelScope Token\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='Qwen/Qwen2.5-72B-Instruct', # ModelScope Model-Id\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful assistant.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '你好'\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd9d36da-d61d-48de-95a8-929b836f3a5e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T18:48:43.303835Z",
     "iopub.status.busy": "2025-04-19T18:48:43.303542Z",
     "iopub.status.idle": "2025-04-19T18:48:43.772803Z",
     "shell.execute_reply": "2025-04-19T18:48:43.772132Z",
     "shell.execute_reply.started": "2025-04-19T18:48:43.303819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'errors': {'message': 'Authenticaton failed, please make sure that a valid ModelScope token is supplied.'}, 'request_id': '5596347d-5b6b-49dd-ab27-81e8ca914651'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m client = OpenAI(\n\u001b[32m      4\u001b[39m     base_url=\u001b[33m'\u001b[39m\u001b[33mhttps://api-inference.modelscope.cn/v1/\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mDASHSCOPE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdeepseek-ai/DeepSeek-R1-Distill-Qwen-7B\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ModelScope Model-Id\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m你好\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m done_reasoning = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'errors': {'message': 'Authenticaton failed, please make sure that a valid ModelScope token is supplied.'}, 'request_id': '5596347d-5b6b-49dd-ab27-81e8ca914651'}"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', # ModelScope Model-Id\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '你好'\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "done_reasoning = False\n",
    "for chunk in response:\n",
    "    reasoning_chunk = chunk.choices[0].delta.reasoning_content\n",
    "    answer_chunk = chunk.choices[0].delta.content\n",
    "    if reasoning_chunk != '':\n",
    "        print(reasoning_chunk, end='',flush=True)\n",
    "    elif answer_chunk != '':\n",
    "        if not done_reasoning:\n",
    "            print('\\n\\n === Final Answer ===\\n')\n",
    "            done_reasoning = True\n",
    "        print(answer_chunk, end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850d038-bc42-496c-ba58-e2519f076b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chatLLM = ChatTongyi(\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2669042-f662-436b-9e83-e690d30e86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-r1\",  # 此处以 deepseek-r1 为例，可按需更换模型名称。\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': contents}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 通过reasoning_content字段打印思考过程\n",
    "print(\"思考过程：\")\n",
    "print(completion.choices[0].message.reasoning_content)\n",
    "\n",
    "# 通过content字段打印最终答案\n",
    "print(\"最终答案：\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa22341-9982-4d24-bb40-09fdff1f8314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5daed4-5eb9-46e9-88cb-c5cac07672c8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T13:58:38.680797Z",
     "iopub.status.busy": "2025-03-27T13:58:38.680444Z",
     "iopub.status.idle": "2025-03-27T13:58:38.685515Z",
     "shell.execute_reply": "2025-03-27T13:58:38.685080Z",
     "shell.execute_reply.started": "2025-03-27T13:58:38.680775Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
    "\n",
    "api_key=\"sk-zfkssxfdpozqzvqakjyumrnkllnxewwasqjdkyvswcvwdrat\"\n",
    "base_url=\"https://api.siliconflow.cn/v1\"\n",
    "\n",
    "    \n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    output1 = ChatOpenAI(\n",
    "       model=\"Pro/deepseek-ai/DeepSeek-R1\",\n",
    "       openai_api_key=api_key,\n",
    "       openai_api_base = base_url,\n",
    "       temperature=0,\n",
    "       max_tokens=8800\n",
    "    ).invoke(prompt_val1)\n",
    "    llm2 = ChatOpenAI(\n",
    "       model=\"Pro/deepseek-ai/DeepSeek-R1\",\n",
    "       openai_api_key=api_key,\n",
    "       openai_api_base = base_url,\n",
    "       temperature=0,\n",
    "       max_tokens=8800\n",
    "    )\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "    chain2 = prompt2 | llm2 | StrOutputParser()\n",
    "    return chain2.invoke({\"joke\": parsed_output1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d414e61c-a603-482f-8398-fbbe503fe5b6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T14:06:37.306551Z",
     "iopub.status.busy": "2025-03-27T14:06:37.306223Z",
     "iopub.status.idle": "2025-03-27T14:07:27.439140Z",
     "shell.execute_reply": "2025-03-27T14:07:27.438671Z",
     "shell.execute_reply.started": "2025-03-27T14:06:37.306531Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subject of the joke is **dogs**, and the humor revolves around a playful pun. The phrase \"two left feet\" is an idiom describing someone who is clumsy or uncoordinated, especially while dancing. By applying this literally to dogs (who, of course, don’t literally have two left feet), the joke creates a whimsical twist. The follow-up line about their \"wagging charm\" adds a lighthearted contrast, acknowledging that even if they were clumsy dancers, their adorable personalities would still win over the audience. 🐶💃"
     ]
    }
   ],
   "source": [
    "for r in custom_chain.stream('dog'):\n",
    "    print(r,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c542621e-d7d8-4b85-84d7-376ea823e36d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T10:15:23.794942Z",
     "iopub.status.busy": "2025-03-30T10:15:23.794634Z",
     "iopub.status.idle": "2025-03-30T10:15:29.525485Z",
     "shell.execute_reply": "2025-03-30T10:15:29.524963Z",
     "shell.execute_reply.started": "2025-03-30T10:15:23.794923Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting faiss-cpu==1.7.3\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/39/eb/1b260c9101fd49789f7ac17c2e2920bf4aa5a6ce0f9ef69dfb4998d8eed4/faiss_cpu-1.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install faiss-cpu==1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ccd4dd-6b08-44a7-8fcf-75a6bf32e540",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T11:26:09.624713Z",
     "iopub.status.busy": "2025-03-30T11:26:09.624425Z",
     "iopub.status.idle": "2025-03-30T11:26:20.180928Z",
     "shell.execute_reply": "2025-03-30T11:26:20.180515Z",
     "shell.execute_reply.started": "2025-03-30T11:26:09.624697Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant_name='江南小厨' food_rating=5 service_rating=3 ambiance_rating=5 recommended_dishes='红烧肉、清蒸鱼' overall_comment='菜品非常美味且环境优雅，具有江南风格装修，值得一去。但服务质量有待提升，建议加强服务员响应速度。'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "class RestaurantReview(BaseModel):\n",
    "    \"\"\"Restaurant review structure\"\"\"\n",
    "\n",
    "    restaurant_name: str = Field(description=\"餐厅的名称\")\n",
    "    food_rating: int = Field(description=\"菜品口味评分，1-5分\")\n",
    "    service_rating: int = Field(description=\"服务质量评分，1-5分\")\n",
    "    ambiance_rating: int = Field(description=\"餐厅环境评分，1-5分\")\n",
    "    recommended_dishes: str = Field(description=\"推荐的特色菜品\")\n",
    "    overall_comment: str = Field(description=\"对餐厅的总体评价和建议\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=RestaurantReview)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一个专业的餐厅评价分析助手，能够从用户描述中提取结构化信息。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"请根据用户的描述，生成一个结构化的餐厅评价。\\n{format_instructions}\\n用户描述: {query}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "model = ChatOpenAI(\n",
    "   model=\"deepseek-r1\",\n",
    "   openai_api_key=api_key[0],\n",
    "   openai_api_base = base_url,\n",
    "   temperature=0,\n",
    "   max_tokens=8800\n",
    ")\n",
    "\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "review_result = chain.invoke(\n",
    "    {\n",
    "        \"query\": \"我昨天去了一家叫'江南小厨'的餐厅，菜品很美味，尤其是红烧肉和清蒸鱼非常好吃。服务员态度一般，有时需要等很久才能叫到人。餐厅装修很有江南风格，环境优雅。总的来说还是值得一去的。\",\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    }\n",
    ")\n",
    "\n",
    "assert isinstance(review_result, RestaurantReview)\n",
    "print(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f802a49-3bac-48c1-b066-35eb1d23324d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T11:26:22.551930Z",
     "iopub.status.busy": "2025-03-30T11:26:22.551648Z",
     "iopub.status.idle": "2025-03-30T11:26:22.554613Z",
     "shell.execute_reply": "2025-03-30T11:26:22.554239Z",
     "shell.execute_reply.started": "2025-03-30T11:26:22.551914Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant_name 江南小厨\n",
      "food_rating 5\n",
      "service_rating 3\n",
      "ambiance_rating 5\n",
      "recommended_dishes 红烧肉、清蒸鱼\n",
      "overall_comment 菜品非常美味且环境优雅，具有江南风格装修，值得一去。但服务质量有待提升，建议加强服务员响应速度。\n"
     ]
    }
   ],
   "source": [
    "for k,v in review_result:\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f1e4fa-908b-4f9f-bb3c-2869eaf08d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T11:21:06.697089Z",
     "iopub.status.busy": "2025-03-30T11:21:06.696765Z",
     "iopub.status.idle": "2025-03-30T11:21:07.218653Z",
     "shell.execute_reply": "2025-03-30T11:21:07.217948Z",
     "shell.execute_reply.started": "2025-03-30T11:21:06.697073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': \"<400> InternalError.Algo.InvalidParameter: 'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error'}, 'id': 'chatcmpl-886df738-d5d5-96c5-8b1e-4b7a362ba681', 'request_id': '886df738-d5d5-96c5-8b1e-4b7a362ba681'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     27\u001b[39m model = ChatOpenAI(\n\u001b[32m     28\u001b[39m    model=\u001b[33m\"\u001b[39m\u001b[33mdeepseek-r1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m    openai_api_key=api_key[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     RestaurantReview\n\u001b[32m     35\u001b[39m )\n\u001b[32m     38\u001b[39m chain = prompt_template | model\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m review_result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m我昨天去了一家叫\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m江南小厨\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m的餐厅，菜品很美味，尤其是红烧肉和清蒸鱼非常好吃。服务员态度一般，有时需要等很久才能叫到人。餐厅装修很有江南风格，环境优雅。总的来说还是值得一去的。\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(review_result, RestaurantReview)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(review_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:3025\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3024\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   3026\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:5358\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5353\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5354\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   5355\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5356\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5357\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5359\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5360\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5361\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5362\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:307\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m     **kwargs: Any,\n\u001b[32m    303\u001b[39m ) -> BaseMessage:\n\u001b[32m    304\u001b[39m     config = ensure_config(config)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    306\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    317\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:843\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    836\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    837\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     **kwargs: Any,\n\u001b[32m    841\u001b[39m ) -> LLMResult:\n\u001b[32m    842\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:933\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    931\u001b[39m         response = \u001b[38;5;28mself\u001b[39m.root_client.beta.chat.completions.parse(**payload)\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_responses_api(payload):\n\u001b[32m    935\u001b[39m     original_schema_obj = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:931\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    929\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    933\u001b[39m     _handle_openai_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py:158\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    153\u001b[39m         response_format=response_format,\n\u001b[32m    154\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    155\u001b[39m         input_tools=tools,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': \"<400> InternalError.Algo.InvalidParameter: 'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error'}, 'id': 'chatcmpl-886df738-d5d5-96c5-8b1e-4b7a362ba681', 'request_id': '886df738-d5d5-96c5-8b1e-4b7a362ba681'}"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class RestaurantReview(BaseModel):\n",
    "    \"\"\"Restaurant review structure\"\"\"\n",
    "\n",
    "    restaurant_name: str = Field(description=\"餐厅的名称\")\n",
    "    food_rating: int = Field(description=\"菜品口味评分，1-5分\")\n",
    "    service_rating: int = Field(description=\"服务质量评分，1-5分\")\n",
    "    ambiance_rating: int = Field(description=\"餐厅环境评分，1-5分\")\n",
    "    recommended_dishes: str = Field(description=\"推荐的特色菜品\")\n",
    "    overall_comment: str = Field(description=\"对餐厅的总体评价和建议\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一个专业的餐厅评价分析助手，能够从用户描述中提取结构化信息。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"请根据用户的描述，生成一个结构化的餐厅评价。\\n用户描述: {query}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "   model=\"deepseek-r1\",\n",
    "   openai_api_key=api_key[0],\n",
    "   openai_api_base = base_url,\n",
    "   temperature=0,\n",
    "   max_tokens=8800\n",
    ").with_structured_output(\n",
    "    RestaurantReview\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt_template | model\n",
    "review_result = chain.invoke(\n",
    "    {\n",
    "        \"query\": \"我昨天去了一家叫'江南小厨'的餐厅，菜品很美味，尤其是红烧肉和清蒸鱼非常好吃。服务员态度一般，有时需要等很久才能叫到人。餐厅装修很有江南风格，环境优雅。总的来说还是值得一去的。\",\n",
    "    }\n",
    ")\n",
    "\n",
    "assert isinstance(review_result, RestaurantReview)\n",
    "print(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a478896-5ff0-4b17-bd62-c86bfac303e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bdcc4-1434-4807-a287-a7eb75d775a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2898fa03-857d-4954-8b49-01ea319eeae8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T12:31:47.746475Z",
     "iopub.status.busy": "2025-03-30T12:31:47.746181Z",
     "iopub.status.idle": "2025-03-30T12:31:47.749172Z",
     "shell.execute_reply": "2025-03-30T12:31:47.748794Z",
     "shell.execute_reply.started": "2025-03-30T12:31:47.746459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16ec86fa-9374-4e6e-9f30-cdf27862401b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T12:06:18.773601Z",
     "iopub.status.busy": "2025-03-30T12:06:18.773333Z",
     "iopub.status.idle": "2025-03-30T12:06:18.776316Z",
     "shell.execute_reply": "2025-03-30T12:06:18.775955Z",
     "shell.execute_reply.started": "2025-03-30T12:06:18.773586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(name=\"capital\", description=\"answer capital to the user's question\"),\n",
    "    ResponseSchema(name=\"source\", description=\"source used to answer the user's question, should be a website.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "021b3dba-e092-41d4-8a31-08ba0bab54a9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T12:16:17.319679Z",
     "iopub.status.busy": "2025-03-30T12:16:17.319394Z",
     "iopub.status.idle": "2025-03-30T12:16:17.322567Z",
     "shell.execute_reply": "2025-03-30T12:16:17.322188Z",
     "shell.execute_reply.started": "2025-03-30T12:16:17.319663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format_instructions = output_parser.get_format_instructions()\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "#     input_variables=[\"question\"],\n",
    "#     partial_variables={\"format_instructions\": format_instructions}\n",
    "# )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\")  \n",
    "    ],\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2803fc19-8523-4724-bdeb-492e4b022644",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T12:31:51.481302Z",
     "iopub.status.busy": "2025-03-30T12:31:51.481026Z",
     "iopub.status.idle": "2025-03-30T12:31:51.483906Z",
     "shell.execute_reply": "2025-03-30T12:31:51.483503Z",
     "shell.execute_reply.started": "2025-03-30T12:31:51.481287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = ChatOpenAI(\n",
    "#    model=\"deepseek-r1\",\n",
    "#    openai_api_key=api_key[0],\n",
    "#    openai_api_base = base_url,\n",
    "#    temperature=0,\n",
    "#    max_tokens=8800\n",
    "# )\n",
    "# model = Tongyi()  \n",
    "\n",
    "model = ChatTongyi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26c3b994-146a-4059-8b45-5ab1389b7797",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T13:32:07.510596Z",
     "iopub.status.busy": "2025-03-30T13:32:07.510314Z",
     "iopub.status.idle": "2025-03-30T13:32:08.270081Z",
     "shell.execute_reply": "2025-03-30T13:32:08.269632Z",
     "shell.execute_reply.started": "2025-03-30T13:32:07.510581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _input = prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "# output = model(_input.to_string())\n",
    "\n",
    "_input = prompt.format_prompt(question=\"中国的首都是哪里？仅输出城市，不要有多余的描述\")\n",
    "output = model(_input.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99d477d9-1011-4eab-bb42-c22c7c16ffbc",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T12:28:51.164541Z",
     "iopub.status.busy": "2025-03-30T12:28:51.164257Z",
     "iopub.status.idle": "2025-03-30T12:28:51.167539Z",
     "shell.execute_reply": "2025-03-30T12:28:51.167172Z",
     "shell.execute_reply.started": "2025-03-30T12:28:51.164526Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: answer the users question as best as possible.\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"answer\": string  // answer to the user\\'s question\\n\\t\"capital\": string  // answer capital to the user\\'s question\\n\\t\"source\": string  // source used to answer the user\\'s question, should be a website.\\n}\\n```\\nwhat\\'s the capital of china?please answer directlly'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48706fd9-5fa5-4dad-8093-f9c5fd882fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T13:32:10.647160Z",
     "iopub.status.busy": "2025-03-30T13:32:10.646885Z",
     "iopub.status.idle": "2025-03-30T13:32:10.650248Z",
     "shell.execute_reply": "2025-03-30T13:32:10.649832Z",
     "shell.execute_reply.started": "2025-03-30T13:32:10.647145Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n\\t\"answer\": \"北京\",\\n\\t\"capital\": \"北京\",\\n\\t\"source\": \"https://www.gov.cn\"\\n}\\n```', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '6543be13-7570-9c0e-8456-0dfd6b5fbb37', 'token_usage': {'input_tokens': 115, 'output_tokens': 31, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 146}}, id='run-2b9103b0-12d6-4e5f-909b-333db801e431-0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2191a513-201d-4faf-9824-d0fc9e8e2204",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T12:34:54.002834Z",
     "iopub.status.busy": "2025-03-30T12:34:54.002552Z",
     "iopub.status.idle": "2025-03-30T12:34:54.006473Z",
     "shell.execute_reply": "2025-03-30T12:34:54.006057Z",
     "shell.execute_reply.started": "2025-03-30T12:34:54.002818Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '中国的首都是北京。', 'capital': '北京', 'source': 'https://www.gov.cn/'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a37ef46-fecc-46da-99e6-3b634f4725e4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-14T12:41:16.292482Z",
     "iopub.status.busy": "2025-04-14T12:41:16.292143Z",
     "iopub.status.idle": "2025-04-14T12:41:16.408758Z",
     "shell.execute_reply": "2025-04-14T12:41:16.408148Z",
     "shell.execute_reply.started": "2025-04-14T12:41:16.292457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_markdown(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    markdown_lines = []\n",
    "\n",
    "    def process_element(el):\n",
    "        tag = el.name\n",
    "\n",
    "        if tag in [f'h{i}' for i in range(1, 7)]:\n",
    "            level = int(tag[1])\n",
    "            markdown_lines.append(f\"{'#' * level} {el.get_text(strip=True)}\")\n",
    "\n",
    "        elif tag == 'p':\n",
    "            markdown_lines.append(el.get_text(strip=True))\n",
    "\n",
    "        elif tag == 'pre':\n",
    "            code = el.get_text()\n",
    "            markdown_lines.append(f\"```\\n{code.strip()}\\n```\")\n",
    "\n",
    "        elif tag == 'code' and el.parent.name != 'pre':\n",
    "            # Inline code\n",
    "            markdown_lines.append(f\"`{el.get_text(strip=True)}`\")\n",
    "\n",
    "        elif tag == 'ul':\n",
    "            for li in el.find_all('li', recursive=False):\n",
    "                markdown_lines.append(f\"- {li.get_text(strip=True)}\")\n",
    "\n",
    "        elif tag == 'ol':\n",
    "            for idx, li in enumerate(el.find_all('li', recursive=False), 1):\n",
    "                markdown_lines.append(f\"{idx}. {li.get_text(strip=True)}\")\n",
    "\n",
    "        elif tag == 'strong':\n",
    "            markdown_lines.append(f\"**{el.get_text(strip=True)}**\")\n",
    "\n",
    "        elif tag == 'em':\n",
    "            markdown_lines.append(f\"*{el.get_text(strip=True)}*\")\n",
    "\n",
    "        # 如果是其他 block 内容，递归解析其子元素\n",
    "        else:\n",
    "            for child in el.children:\n",
    "                if isinstance(child, str):\n",
    "                    markdown_lines.append(child.strip())\n",
    "                elif hasattr(child, 'name'):\n",
    "                    process_element(child)\n",
    "\n",
    "    # 处理 body 的所有直接子元素\n",
    "    for el in soup.body.contents if soup.body else soup.contents:\n",
    "        if hasattr(el, 'name'):\n",
    "            process_element(el)\n",
    "\n",
    "    return \"\\n\\n\".join([line for line in markdown_lines if line.strip()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5371e35f-f16d-4fb8-8697-ffa92aaa0b09",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-14T13:02:56.557302Z",
     "iopub.status.busy": "2025-04-14T13:02:56.556795Z",
     "iopub.status.idle": "2025-04-14T13:02:56.564380Z",
     "shell.execute_reply": "2025-04-14T13:02:56.563886Z",
     "shell.execute_reply.started": "2025-04-14T13:02:56.557280Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# !pip install markdownify\n",
    "from markdownify import markdownify\n",
    "\n",
    "html = markdownify(\"<h1>Hello, World!</h1>\")\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f8caab-b9be-4e70-a02e-8403b55df70a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-14T13:03:05.818965Z",
     "iopub.status.busy": "2025-04-14T13:03:05.818655Z",
     "iopub.status.idle": "2025-04-14T13:03:05.843313Z",
     "shell.execute_reply": "2025-04-14T13:03:05.842259Z",
     "shell.execute_reply.started": "2025-04-14T13:03:05.818944Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'turndown'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mturndown\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TurndownService\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhtml_to_markdown\u001b[39m(html_content):\n\u001b[32m      4\u001b[39m     td = TurndownService()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'turndown'"
     ]
    }
   ],
   "source": [
    "\n",
    "from turndown import TurndownService\n",
    "\n",
    "def html_to_markdown(html_content):\n",
    "    td = TurndownService()\n",
    "    # 自定义规则（可选）\n",
    "    td.add_rule('code', {\n",
    "        'filter': ['pre'],\n",
    "        'replacement': lambda content: f'```\\n{content}\\n```'\n",
    "    })\n",
    "    return td.turndown(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb88b8-bed1-4769-866c-d3ef8d92fe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fe61414-c1bd-412a-9f9a-36c43caf2439",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-14T13:39:21.377968Z",
     "iopub.status.busy": "2025-04-14T13:39:21.377655Z",
     "iopub.status.idle": "2025-04-14T13:39:21.390825Z",
     "shell.execute_reply": "2025-04-14T13:39:21.390319Z",
     "shell.execute_reply.started": "2025-04-14T13:39:21.377947Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的运行日志，我来分析Qwen-72B在Kaggle双卡T4上的实际运行情况：\n",
      "\n",
      "### 加载过程分析\n",
      "\n",
      "1. **模型加载时间**：完整的模型加载耗时约5分42秒（从\"Loading safetensors checkpoint shards\"开始到结束）这表明模型采用了分片存储（9个分片），每个分片加载时间在30-45秒\n",
      "\n",
      "4. **内存使用情况**：`Loading model weights took 11.9423 GB`日志显示加载模型权重占用了约12GB内存，这与预期相符，因为使用了AWQ量化技术\n",
      "\n",
      "6. **GPU资源分配**：`# GPU blocks: 325, # CPU blocks: 409Maximum concurrency for 2048 tokens per request: 2.54x`vLLM为模型分配了325个GPU内存块和409个CPU内存块每个2048 tokens请求的最大并发度为2.54倍\n",
      "\n",
      "### 推理性能分析\n",
      "\n",
      "1. **推理速度**：`Processed prompts: 100%|█| 12/12 [00:40<00:00, 3.34s/it]`处理12个提示总共花了40秒，平均每个提示处理时间约3.34秒估计输入吞吐量约165 tokens/秒（这是输入处理速度，不是生成速度）\n",
      "\n",
      "4. **总运行时间**：`Wall time: 7min 27s`整个过程（包括模型加载和推理）总共耗时7分27秒\n",
      "\n",
      "6. **使用了AWQ量化**：`WARNING 12-08 18:57:34 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.`日志显示使用了AWQ量化，并警告说该量化方法尚未完全优化，速度可能比非量化模型慢\n",
      "\n",
      "8. **注意到使用Xformers后端**：`INFO 12-08 18:57:35 selector.py:115] Using XFormers backend.`由于T4 GPU是Turing架构，不支持FlashAttention-2，系统自动选择了Xformers作为注意力机制后端\n",
      "\n",
      "### 部署优化分析\n",
      "\n",
      "1. **量化技术**：\n",
      "代码中只提到了`quantization=\"awq\"`的注释，但日志显示确实使用了AWQ量化，这是4位量化技术，可以大幅减少模型的内存占用\n",
      "\n",
      "2. **P2P通信限制**：`Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed`T4 GPU的P2P通信能力受限，导致自定义allreduce被禁用，这可能会影响多GPU协同工作效率\n",
      "\n",
      "4. **VRAM使用情况**：\n",
      "虽然代码中设置了`gpu_memory_utilization=0.99`，但日志没有直接显示VRAM使用率，根据加载数据可推测：T4 GPU每卡16GB VRAM12GB用于模型权重剩余约20GB（两卡合计）用于KV缓存、激活值和中间计算结果\n",
      "\n",
      "### 实际性能结论\n",
      "\n",
      "1. **单样本处理时间**：约3.34秒/样本这比我之前预估的0.5-1秒慢，主要是因为:模型使用了AWQ量化，日志中明确提示量化可能导致性能下降实际推理上下文长度可能比预期长（样本包含数学题和思考过程）\n",
      "\n",
      "5. **吞吐量**：约18个样本/分钟这对于资源受限的T4 GPU来说已经是不错的性能\n",
      "\n",
      "7. **量化效果**：AWQ量化成功将72B参数模型压缩到可以在双T4上运行但量化导致了一定的速度损失\n",
      "\n",
      "10. **加载与推理时间对比**：模型加载时间(5分42秒)占总运行时间(7分27秒)的大部分实际推理只占约40秒\n",
      "\n",
      "在这种配置下，Qwen-72B确实能在双T4上运行，但大部分时间花在模型加载上，实际推理速度约为3.34秒/样本。这对于批量处理任务或非实时应用来说是可接受的，但对于需要实时响应的应用仍然太慢。\n"
     ]
    }
   ],
   "source": [
    "def load_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "    \n",
    "html_input = load_text_file(\"htmlfile.txt\")\n",
    "\n",
    "markdown = html_to_markdown(html_input)\n",
    "print(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc1e21aa-4d7c-4aa4-a4ba-d9f2a3b88332",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-14T13:06:47.499448Z",
     "iopub.status.busy": "2025-04-14T13:06:47.498937Z",
     "iopub.status.idle": "2025-04-14T13:06:47.508000Z",
     "shell.execute_reply": "2025-04-14T13:06:47.507496Z",
     "shell.execute_reply.started": "2025-04-14T13:06:47.499422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def html_to_markdown(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    markdown_lines = []\n",
    "\n",
    "    def render_inline(el):\n",
    "        if isinstance(el, NavigableString):\n",
    "            return el.strip()\n",
    "\n",
    "        if el.name == \"strong\":\n",
    "            return f\"**{''.join(render_inline(c) for c in el.contents)}**\"\n",
    "        elif el.name == \"em\":\n",
    "            return f\"*{''.join(render_inline(c) for c in el.contents)}*\"\n",
    "        elif el.name == \"code\":\n",
    "            return f\"`{''.join(render_inline(c) for c in el.contents)}`\"\n",
    "        else:\n",
    "            return ''.join(render_inline(c) for c in el.contents)\n",
    "\n",
    "    def render_block(el):\n",
    "        if isinstance(el, NavigableString):\n",
    "            text = el.strip()\n",
    "            if text:\n",
    "                markdown_lines.append(text)\n",
    "            return\n",
    "\n",
    "        tag = el.name\n",
    "\n",
    "        if tag in [f'h{i}' for i in range(1, 7)]:\n",
    "            level = int(tag[1])\n",
    "            text = render_inline(el)\n",
    "            markdown_lines.append(f\"{'#' * level} {text}\")\n",
    "\n",
    "        elif tag == \"p\":\n",
    "            markdown_lines.append(render_inline(el))\n",
    "\n",
    "        elif tag == \"li\":\n",
    "            content = render_inline(el)\n",
    "            parent = el.find_parent()\n",
    "            if parent and parent.name == \"ol\":\n",
    "                index = list(parent.find_all(\"li\")).index(el) + 1\n",
    "                markdown_lines.append(f\"{index}. {content}\")\n",
    "            else:\n",
    "                markdown_lines.append(f\"- {content}\")\n",
    "\n",
    "        elif tag in [\"ul\", \"ol\"]:\n",
    "            for child in el.find_all(\"li\", recursive=False):\n",
    "                render_block(child)\n",
    "\n",
    "        elif tag == \"pre\":\n",
    "            code_text = el.get_text().strip()\n",
    "            markdown_lines.append(f\"```\\n{code_text}\\n```\")\n",
    "\n",
    "        elif tag == \"code\" and el.parent.name != \"pre\":\n",
    "            markdown_lines.append(render_inline(el))\n",
    "\n",
    "        else:\n",
    "            for child in el.children:\n",
    "                render_block(child)\n",
    "\n",
    "    body = soup.body or soup  # fallback to root\n",
    "    for child in body.children:\n",
    "        render_block(child)\n",
    "\n",
    "    return \"\\n\\n\".join(line for line in markdown_lines if line.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa37f02-c506-4806-be96-5ba34b957147",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08bd444b-d3bc-42be-9019-28aa66c92583",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-14T13:59:41.667255Z",
     "iopub.status.busy": "2025-04-14T13:59:41.666827Z",
     "iopub.status.idle": "2025-04-14T13:59:41.678351Z",
     "shell.execute_reply": "2025-04-14T13:59:41.677766Z",
     "shell.execute_reply.started": "2025-04-14T13:59:41.667222Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于提供的日志，我来详细分析模型占用情况和推理时间：\n",
      "\n",
      "### 模型占用分析\n",
      "\n",
      "**1. 显存(VRAM)占用：**\n",
      "\n",
      "* 每张T4 GPU有16GB VRAM，共32GB (双卡)\n",
      "* 日志显示：`Loading model weights took 11.9423 GB`，这表示模型权重本身占用了约12GB\n",
      "* vLLM分配了`# GPU blocks: 325`个GPU内存块，用于KV缓存和计算\n",
      "* 推测总显存占用：每卡约14-15GB (接近设定的`gpu_memory_utilization=0.99`，约16GB×0.99=15.84GB)\n",
      "\n",
      "**2. 内存(RAM)占用：**\n",
      "\n",
      "* 日志显示分配了`# CPU blocks: 409`个CPU内存块\n",
      "* 设置了`cpu_offload_gb=8`，表示约8GB模型参数被卸载到CPU内存\n",
      "* 预估总RAM占用：约10-12GB (包括卸载的模型参数和工作内存)\n",
      "\n",
      "**3. 硬盘占用：**\n",
      "\n",
      "* 日志显示加载了9个模型分片：`Loading safetensors checkpoint shards: 100% Completed | 9/9`\n",
      "* 设置了`swap_space=1`，表示额外1GB硬盘空间用作交换\n",
      "* AWQ量化后的模型大小预估：约18-20GB (基于9个分片的加载时间和普通的AWQ量化比例)\n",
      "* 实际硬盘占用：约20-22GB (包括模型参数和交换空间)\n",
      "\n",
      "**4. 量化压缩比：**\n",
      "\n",
      "* 原始72B参数模型理论大小：~144GB (FP16格式)\n",
      "* AWQ量化后约18-20GB，压缩比约为7-8倍\n",
      "\n",
      "### 推理时间分析\n",
      "\n",
      "日志中的`Wall time: 7min 27s`是**整个执行过程**的总时间，包括：\n",
      "\n",
      "1. **模型加载时间**：约5分42秒\n",
      "\n",
      "   ```\n",
      "\n",
      "\n",
      "   ```\n",
      "   Loading safetensors checkpoint shards: 100% Completed | 9/9 [05:42<00:00, 38.00s/it]\n",
      "   ```\n",
      "\n",
      "\n",
      "   ```\n",
      "2. **实际推理时间**：约40秒\n",
      "\n",
      "   ```\n",
      "\n",
      "\n",
      "   ```\n",
      "   Processed prompts: 100%|█| 12/12 [00:40<00:00, 3.34s/it]\n",
      "   ```\n",
      "\n",
      "\n",
      "   ```\n",
      "3. **其他开销**：约1分钟(初始化、资源分配、结果处理等)\n",
      "\n",
      "所以，7分27秒是**完整执行时间**，其中大部分(76%)用于模型加载，实际推理只占了约9%的时间。\n",
      "\n",
      "这表明如果需要多次使用该模型，保持模型常驻内存会大大提高效率。12个样本的小批量只需40秒完成推理，平均每个样本3.34秒，吞吐量约为18个样本/分钟。\n",
      "\n",
      "如果是处理大规模数据集，模型加载的一次性成本会被分摊，整体效率会提高。例如，处理1000个样本可能需要约60分钟(模型加载6分钟+推理54分钟)，而不是7分钟×1000÷12=583分钟。\n"
     ]
    }
   ],
   "source": [
    "# print(html_to_markdown(html_input))\n",
    "html_input = load_text_file(\"htmlfile.txt\")\n",
    "print(markdownify(html_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d7c3d-59b4-4401-9334-454f2adf10c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
