{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1771224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CFG:\n",
    "    input_path = \"/content/drive/MyDrive/RetrieverNLP/resource/input/\"\n",
    "    train_path = f\"{input_path}train.csv\"\n",
    "    test_path  = f\"{input_path}test.csv\"\n",
    "    misc_path  = f\"{input_path}misconception_mapping.csv\"\n",
    "    samp_path  = f\"{input_path}sample_submission.csv\"\n",
    "#     max_cutoff = 50 #V1\n",
    "    #max_cutoff = 100 #V2\n",
    "    #max_cutoff = 50 #v3\n",
    "    max_cutoff = 150 #v3\n",
    "    \n",
    "    is_train   = False\n",
    "    if is_train:\n",
    "        embd_name  = \"BAAI/bge-large-en-v1.5\"#online\n",
    "        rerank_na  = 'BAAI/bge-reranker-large'#online\n",
    "    else:\n",
    "        embd_name  = \"/kaggle/input/bge-large-en-v1-5/bge-large-en-v1.5\"#offline\n",
    "        rerank_na  = \"/kaggle/input/bge-reranker-large\"#offline\n",
    "        \n",
    "    with_fineture_reranker = True\n",
    "    reranker_fineture_path = \"/kaggle/input/bge-reranker-ft-v2\"\n",
    "#     reranker_fineture_path = \"/kaggle/input/bge-reranker-ft-v3\"\n",
    "cfg = CFG()\n",
    "\n",
    "\n",
    "train                 = pd.read_csv(cfg.train_path)\n",
    "misconception_mapping = pd.read_csv(cfg.misc_path)\n",
    "\n",
    "\n",
    "\n",
    "def make_all_question_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"all_question_text\"] = df[\"ConstructName\"] +\" \" +df[\"QuestionText\"]\n",
    "    return df\n",
    "train = make_all_question_text(train)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 第一步：处理答案文本（AnswerXText）\n",
    "    text_long = pd.melt(\n",
    "        df,\n",
    "        id_vars    = [\"QuestionId\", \"all_question_text\", \"CorrectAnswer\"],\n",
    "        value_vars = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"],\n",
    "        var_name   = 'Answer',\n",
    "        value_name = 'AnswerText'\n",
    "    )\n",
    "    # 提取答案选项字母（如 A/B/C/D）\n",
    "    text_long['Answer'] = text_long['Answer'].str.replace('Answer', '').str.replace('Text', '')\n",
    "\n",
    "    # 第二步：处理错误概念ID（MisconceptionXId）\n",
    "    misconception_long = pd.melt(\n",
    "        df,\n",
    "        id_vars    = [\"QuestionId\"],\n",
    "        value_vars = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"],\n",
    "        var_name   = 'MisconceptionAnswer',\n",
    "        value_name = 'MisconceptionId'\n",
    "    )\n",
    "    # 提取答案选项字母（如 A/B/C/D）\n",
    "    misconception_long['Answer'] = misconception_long['MisconceptionAnswer'].str.replace('Misconception', '').str.replace('Id', '')\n",
    "    misconception_long = misconception_long.drop(columns=['MisconceptionAnswer'])\n",
    "\n",
    "    # 合并两个长格式数据\n",
    "    merged_long = pd.merge(\n",
    "        text_long,\n",
    "        misconception_long,\n",
    "        on=['QuestionId', 'Answer'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return merged_long\n",
    "\n",
    "train_long = wide_to_long(train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ======================================================\n",
    "# 配置区（按需修改）\n",
    "# ======================================================\n",
    "MODELS_TO_TEST = {\n",
    "    \"NV-Embed-v2\": {\n",
    "        \"model_name\": \"nvidia/NV-Embed-v2\",\n",
    "        \"query_instruction\": \"\",  # NV嵌入不需要指令前缀\n",
    "        \"normalize\": True,\n",
    "        \"trust_remote_code\":True\n",
    "    },\n",
    "    \"BGE-large-en\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"query_instruction\": \"Represent this sentence for searching relevant passages: \",\n",
    "        \"normalize\": True,\n",
    "        \"trust_remote_code\":False\n",
    "    },\n",
    "    \"BGE-M3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"query_instruction\": \"\",\n",
    "        \"normalize\": False , # M3自带归一化\n",
    "        \"trust_remote_code\":False\n",
    "    }\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PER_GPU_BATCH_SIZE = 32\n",
    "\n",
    "# ======================================================\n",
    "# 核心评估函数\n",
    "# ======================================================\n",
    "def evaluate_model(model_config, misconception_mapping, train_data):\n",
    "    \"\"\"评估单个模型的Recall@50\"\"\"\n",
    "    # 加载模型与分词器\n",
    "\n",
    "    model_path = \"./nv-embed-v2\"\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    if model_name == \"nvidia/NV-Embed-v2\":\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        ).to(DEVICE)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True    \n",
    "        ).to(DEVICE)\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_config[\"model_name\"]\n",
    "        ).to(DEVICE)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_config[\"model_name\"]).to(DEVICE)\n",
    "    \n",
    "    # 构建候选池索引映射\n",
    "    misconception_ids = misconception_mapping['MisconceptionId'].values.tolist()\n",
    "    misconception_texts = misconception_mapping['MisconceptionName'].values.tolist()\n",
    "    id_to_index = {_id: idx for idx, _id in enumerate(misconception_ids)}\n",
    "\n",
    "    # 生成候选池向量\n",
    "    def encode_texts(texts, is_query=False):\n",
    "        all_vectors = []\n",
    "        for i in tqdm(range(0, len(texts), PER_GPU_BATCH_SIZE)):\n",
    "            batch_texts = texts[i:i+PER_GPU_BATCH_SIZE]\n",
    "            \n",
    "            # 添加指令前缀（针对BGE）\n",
    "            if is_query and model_config[\"query_instruction\"]:\n",
    "                batch_texts = [model_config[\"query_instruction\"] + t for t in batch_texts]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state[:, 0]  # 取CLS token\n",
    "            \n",
    "            if model_config[\"normalize\"]:\n",
    "                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "            all_vectors.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.concatenate(all_vectors, axis=0)\n",
    "    \n",
    "    # 编码候选池\n",
    "    sentence_embeddings = encode_texts(misconception_texts)\n",
    "    \n",
    "    # 编码查询\n",
    "    query_texts = [item[\"all_text\"] for item in train_data]\n",
    "    ground_truth_indices = [id_to_index[item[\"MisconceptionId\"]] for item in train_data]\n",
    "    query_embeddings = encode_texts(query_texts, is_query=True)\n",
    "    \n",
    "    # 计算Recall@50\n",
    "    def calculate_recall(sent_emb, query_emb, gt_indices, top_k=50):\n",
    "        index = faiss.IndexFlatIP(sent_emb.shape[1])\n",
    "        index.add(sent_emb.astype(np.float32))\n",
    "        \n",
    "        _, top_indices = index.search(query_emb.astype(np.float32), top_k)\n",
    "        return np.mean([1 if gt in indices else 0 for gt, indices in zip(gt_indices, top_indices)])\n",
    "    \n",
    "    return calculate_recall(sentence_embeddings, query_embeddings, ground_truth_indices)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 执行测试\n",
    "# ======================================================\n",
    "\n",
    "    # 加载数据（需替换为实际数据）\n",
    "    # misconception_mapping = ... \n",
    "    # train_data = ...\n",
    "\n",
    "\n",
    "results = {}\n",
    "for model_name, config in MODELS_TO_TEST.items():\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    recall = evaluate_model(config, misconception_mapping, train_long)\n",
    "    results[model_name] = recall\n",
    "    print(f\"{model_name} Recall@50: {recall*100:.2f}%\")\n",
    "\n",
    "# 打印最终对比结果\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for model, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model:<15} | Recall@50: {score*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
